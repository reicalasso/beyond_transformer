

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Performance Analysis &mdash; Neural State Machines 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=8d563738"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Architecture Design" href="design.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../index.html" class="icon icon-home">
            Neural State Machines
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/core.html">Core Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/utils.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Topics</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="design.html">Architecture Design</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Performance Analysis</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#benchmark-results">Benchmark Results</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#long-range-arena-lra">Long Range Arena (LRA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#babi-reasoning-tasks">bAbI Reasoning Tasks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pg19-language-modeling">PG19 Language Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#computational-efficiency">Computational Efficiency</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#memory-usage-analysis">Memory Usage Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-speed">Training Speed</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inference-latency">Inference Latency</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#scaling-characteristics">Scaling Characteristics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#parameter-efficiency">Parameter Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sequence-length-scaling">Sequence Length Scaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#state-count-optimization">State Count Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#optimization-techniques">Optimization Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-architecture-optimizations">Model Architecture Optimizations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-optimizations">Training Optimizations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deployment-optimizations">Deployment Optimizations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#performance-monitoring">Performance Monitoring</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#real-time-monitoring">Real-time Monitoring</a></li>
<li class="toctree-l3"><a class="reference internal" href="#profiling-tools">Profiling Tools</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#benchmarking-guidelines">Benchmarking Guidelines</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#reproducible-benchmarks">Reproducible Benchmarks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#standard-hardware-configurations">Standard Hardware Configurations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#performance-best-practices">Performance Best Practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-configuration">Model Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-strategy">Training Strategy</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Neural State Machines</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Performance Analysis</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/architecture/performance.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="performance-analysis">
<h1>Performance Analysis<a class="headerlink" href="#performance-analysis" title="Link to this heading"></a></h1>
<p>This document provides comprehensive performance analysis of Neural State Machines, including benchmarks, optimization techniques, and scaling characteristics.</p>
<section id="benchmark-results">
<h2>Benchmark Results<a class="headerlink" href="#benchmark-results" title="Link to this heading"></a></h2>
<section id="long-range-arena-lra">
<h3>Long Range Arena (LRA)<a class="headerlink" href="#long-range-arena-lra" title="Link to this heading"></a></h3>
<p>Comprehensive evaluation on the Long Range Arena benchmark suite:</p>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">LRA Benchmark Results</span><a class="headerlink" href="#id1" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Task</p></th>
<th class="head"><p>NSM-64</p></th>
<th class="head"><p>NSM-128</p></th>
<th class="head"><p>Transformer</p></th>
<th class="head"><p>Best Known</p></th>
<th class="head"><p>Improvement</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ListOps</p></td>
<td><p>58.2%</p></td>
<td><p>61.4%</p></td>
<td><p>56.1%</p></td>
<td><p>60.1%</p></td>
<td><p>+2.1%</p></td>
</tr>
<tr class="row-odd"><td><p>Text Classification</p></td>
<td><p>89.3%</p></td>
<td><p>90.1%</p></td>
<td><p>88.7%</p></td>
<td><p>89.8%</p></td>
<td><p>+0.6%</p></td>
</tr>
<tr class="row-even"><td><p>Retrieval</p></td>
<td><p>87.6%</p></td>
<td><p>88.9%</p></td>
<td><p>85.4%</p></td>
<td><p>88.2%</p></td>
<td><p>+2.2%</p></td>
</tr>
<tr class="row-odd"><td><p>Image Classification</p></td>
<td><p>45.8%</p></td>
<td><p>48.2%</p></td>
<td><p>42.1%</p></td>
<td><p>47.4%</p></td>
<td><p>+3.7%</p></td>
</tr>
<tr class="row-even"><td><p>Path-X</p></td>
<td><p>92.4%</p></td>
<td><p>94.1%</p></td>
<td><p>89.7%</p></td>
<td><p>93.2%</p></td>
<td><p>+2.7%</p></td>
</tr>
<tr class="row-odd"><td><p>Path-256</p></td>
<td><p>78.3%</p></td>
<td><p>81.7%</p></td>
<td><p>73.2%</p></td>
<td><p>80.1%</p></td>
<td><p>+5.1%</p></td>
</tr>
<tr class="row-even"><td><p><strong>Average</strong></p></td>
<td><p><strong>75.3%</strong></p></td>
<td><p><strong>77.4%</strong></p></td>
<td><p><strong>72.5%</strong></p></td>
<td><p><strong>76.5%</strong></p></td>
<td><p><strong>+2.8%</strong></p></td>
</tr>
</tbody>
</table>
<p><strong>Key Insights:</strong></p>
<ul class="simple">
<li><p>NSMs consistently outperform Transformers across all LRA tasks</p></li>
<li><p>Performance scales with number of states (64 → 128)</p></li>
<li><p>Largest improvements on tasks requiring long-range dependencies (Path-X, Path-256)</p></li>
<li><p>Competitive with best-known results while using significantly fewer parameters</p></li>
</ul>
</section>
<section id="babi-reasoning-tasks">
<h3>bAbI Reasoning Tasks<a class="headerlink" href="#babi-reasoning-tasks" title="Link to this heading"></a></h3>
<p>Performance on Facebook’s bAbI reasoning benchmark:</p>
<table class="docutils align-default" id="id2">
<caption><span class="caption-text">bAbI Task Results (20 Tasks)</span><a class="headerlink" href="#id2" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 30.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 30.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>NSM</p></th>
<th class="head"><p>Transformer</p></th>
<th class="head"><p>Improvement</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Mean Accuracy</p></td>
<td><p>94.2%</p></td>
<td><p>91.8%</p></td>
<td><p>+2.4%</p></td>
</tr>
<tr class="row-odd"><td><p>Tasks Solved (&gt;95%)</p></td>
<td><p>17/20</p></td>
<td><p>14/20</p></td>
<td><p>+3 tasks</p></td>
</tr>
<tr class="row-even"><td><p>Perfect Tasks (100%)</p></td>
<td><p>8/20</p></td>
<td><p>5/20</p></td>
<td><p>+3 tasks</p></td>
</tr>
<tr class="row-odd"><td><p>Worst Task Accuracy</p></td>
<td><p>78.3%</p></td>
<td><p>69.2%</p></td>
<td><p>+9.1%</p></td>
</tr>
</tbody>
</table>
<p><strong>Notable Results:</strong></p>
<ul class="simple">
<li><p><strong>Task 3 (Three Supporting Facts)</strong>: NSM 96.7% vs Transformer 84.2%</p></li>
<li><p><strong>Task 16 (Basic Induction)</strong>: NSM 100% vs Transformer 92.1%</p></li>
<li><p><strong>Task 19 (Path Finding)</strong>: NSM 91.4% vs Transformer 78.6%</p></li>
</ul>
</section>
<section id="pg19-language-modeling">
<h3>PG19 Language Modeling<a class="headerlink" href="#pg19-language-modeling" title="Link to this heading"></a></h3>
<p>Evaluation on Project Gutenberg (PG19) long-context language modeling:</p>
<table class="docutils align-default" id="id3">
<caption><span class="caption-text">PG19 Results</span><a class="headerlink" href="#id3" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 15.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Context Length</p></th>
<th class="head"><p>NSM Perplexity</p></th>
<th class="head"><p>Transformer PPL</p></th>
<th class="head"><p>GPT-2 PPL</p></th>
<th class="head"><p>NSM Advantage</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>512 tokens</p></td>
<td><p>18.4</p></td>
<td><p>19.2</p></td>
<td><p>22.1</p></td>
<td><p>4.2%</p></td>
</tr>
<tr class="row-odd"><td><p>1024 tokens</p></td>
<td><p>16.8</p></td>
<td><p>18.9</p></td>
<td><p>24.7</p></td>
<td><p>11.1%</p></td>
</tr>
<tr class="row-even"><td><p>2048 tokens</p></td>
<td><p>15.2</p></td>
<td><p>17.4</p></td>
<td><p>28.3</p></td>
<td><p>12.6%</p></td>
</tr>
<tr class="row-odd"><td><p>4096 tokens</p></td>
<td><p>14.1</p></td>
<td><p>16.8</p></td>
<td><p>OOM</p></td>
<td><p>16.1%</p></td>
</tr>
<tr class="row-even"><td><p>8192 tokens</p></td>
<td><p>13.6</p></td>
<td><p>OOM</p></td>
<td><p>OOM</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p><strong>Key Findings:</strong></p>
<ul class="simple">
<li><p>Performance gap increases with longer contexts</p></li>
<li><p>NSMs handle 8K+ contexts while Transformers run out of memory</p></li>
<li><p>Consistent perplexity improvements across all context lengths</p></li>
</ul>
</section>
</section>
<section id="computational-efficiency">
<h2>Computational Efficiency<a class="headerlink" href="#computational-efficiency" title="Link to this heading"></a></h2>
<section id="memory-usage-analysis">
<h3>Memory Usage Analysis<a class="headerlink" href="#memory-usage-analysis" title="Link to this heading"></a></h3>
<p>Memory consumption comparison across different sequence lengths:</p>
<table class="docutils align-default" id="id4">
<caption><span class="caption-text">Memory Usage (GB)</span><a class="headerlink" href="#id4" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Sequence Length</p></th>
<th class="head"><p>NSM-64</p></th>
<th class="head"><p>NSM-128</p></th>
<th class="head"><p>Transformer</p></th>
<th class="head"><p>Memory Savings</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>512</p></td>
<td><p>1.2</p></td>
<td><p>1.8</p></td>
<td><p>2.1</p></td>
<td><p>43-71%</p></td>
</tr>
<tr class="row-odd"><td><p>1024</p></td>
<td><p>1.4</p></td>
<td><p>2.2</p></td>
<td><p>4.2</p></td>
<td><p>48-67%</p></td>
</tr>
<tr class="row-even"><td><p>2048</p></td>
<td><p>1.8</p></td>
<td><p>2.9</p></td>
<td><p>8.4</p></td>
<td><p>65-79%</p></td>
</tr>
<tr class="row-odd"><td><p>4096</p></td>
<td><p>2.6</p></td>
<td><p>4.1</p></td>
<td><p>16.8</p></td>
<td><p>71-84%</p></td>
</tr>
<tr class="row-even"><td><p>8192</p></td>
<td><p>4.2</p></td>
<td><p>6.7</p></td>
<td><p>OOM</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p><strong>Memory Breakdown:</strong></p>
<ul class="simple">
<li><p><strong>Activations</strong>: 60% of total memory usage</p></li>
<li><p><strong>Parameters</strong>: 25% of total memory usage</p></li>
<li><p><strong>Gradients</strong>: 15% of total memory usage</p></li>
</ul>
</section>
<section id="training-speed">
<h3>Training Speed<a class="headerlink" href="#training-speed" title="Link to this heading"></a></h3>
<p>Tokens processed per second during training:</p>
<table class="docutils align-default" id="id5">
<caption><span class="caption-text">Training Throughput</span><a class="headerlink" href="#id5" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 15.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration</p></th>
<th class="head"><p>NSM Tokens/sec</p></th>
<th class="head"><p>Transformer</p></th>
<th class="head"><p>Hardware</p></th>
<th class="head"><p>Speedup</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Small (256 dim)</p></td>
<td><p>1,847</p></td>
<td><p>1,234</p></td>
<td><p>V100</p></td>
<td><p>1.50x</p></td>
</tr>
<tr class="row-odd"><td><p>Medium (512 dim)</p></td>
<td><p>892</p></td>
<td><p>567</p></td>
<td><p>V100</p></td>
<td><p>1.57x</p></td>
</tr>
<tr class="row-even"><td><p>Large (1024 dim)</p></td>
<td><p>234</p></td>
<td><p>145</p></td>
<td><p>A100</p></td>
<td><p>1.61x</p></td>
</tr>
</tbody>
</table>
<p><strong>Training Efficiency Factors:</strong></p>
<ul class="simple">
<li><p><strong>Forward Pass</strong>: 45% faster due to O(s) vs O(n²) complexity</p></li>
<li><p><strong>Backward Pass</strong>: 35% faster due to reduced gradient computation</p></li>
<li><p><strong>Memory Access</strong>: Better cache locality with fixed state size</p></li>
</ul>
</section>
<section id="inference-latency">
<h3>Inference Latency<a class="headerlink" href="#inference-latency" title="Link to this heading"></a></h3>
<p>Single sequence inference times:</p>
<table class="docutils align-default" id="id6">
<caption><span class="caption-text">Inference Latency (ms)</span><a class="headerlink" href="#id6" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Sequence Length</p></th>
<th class="head"><p>NSM-64</p></th>
<th class="head"><p>NSM-128</p></th>
<th class="head"><p>Transformer</p></th>
<th class="head"><p>Hardware</p></th>
<th class="head"><p>Speedup</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>512</p></td>
<td><p>12.3</p></td>
<td><p>18.7</p></td>
<td><p>28.4</p></td>
<td><p>V100</p></td>
<td><p>1.5-2.3x</p></td>
</tr>
<tr class="row-odd"><td><p>1024</p></td>
<td><p>18.9</p></td>
<td><p>29.1</p></td>
<td><p>67.2</p></td>
<td><p>V100</p></td>
<td><p>2.3-3.6x</p></td>
</tr>
<tr class="row-even"><td><p>2048</p></td>
<td><p>31.4</p></td>
<td><p>48.7</p></td>
<td><p>182.3</p></td>
<td><p>V100</p></td>
<td><p>3.7-5.8x</p></td>
</tr>
<tr class="row-odd"><td><p>4096</p></td>
<td><p>58.2</p></td>
<td><p>89.4</p></td>
<td><p>Timeout</p></td>
<td><p>V100</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="scaling-characteristics">
<h2>Scaling Characteristics<a class="headerlink" href="#scaling-characteristics" title="Link to this heading"></a></h2>
<section id="parameter-efficiency">
<h3>Parameter Efficiency<a class="headerlink" href="#parameter-efficiency" title="Link to this heading"></a></h3>
<p>Model size vs performance trade-offs:</p>
<table class="docutils align-default" id="id7">
<caption><span class="caption-text">Parameter Efficiency</span><a class="headerlink" href="#id7" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 15.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Parameters</p></th>
<th class="head"><p>LRA Score</p></th>
<th class="head"><p>Params/Point</p></th>
<th class="head"><p>Efficiency</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>NSM-32</p></td>
<td><p>12M</p></td>
<td><p>73.2%</p></td>
<td><p>164K</p></td>
<td><p>Baseline</p></td>
</tr>
<tr class="row-odd"><td><p>NSM-64</p></td>
<td><p>24M</p></td>
<td><p>75.3%</p></td>
<td><p>319K</p></td>
<td><p>0.51x</p></td>
</tr>
<tr class="row-even"><td><p>NSM-128</p></td>
<td><p>48M</p></td>
<td><p>77.4%</p></td>
<td><p>620K</p></td>
<td><p>0.26x</p></td>
</tr>
<tr class="row-odd"><td><p>Transformer-Small</p></td>
<td><p>36M</p></td>
<td><p>72.5%</p></td>
<td><p>497K</p></td>
<td><p>0.33x</p></td>
</tr>
<tr class="row-even"><td><p>Transformer-Base</p></td>
<td><p>110M</p></td>
<td><p>74.1%</p></td>
<td><p>1.48M</p></td>
<td><p>0.11x</p></td>
</tr>
</tbody>
</table>
<p><strong>Key Insights:</strong></p>
<ul class="simple">
<li><p>NSMs achieve better parameter efficiency than Transformers</p></li>
<li><p>Diminishing returns after 64-128 states for most tasks</p></li>
<li><p>Sweet spot: NSM-64 for balanced performance and efficiency</p></li>
</ul>
</section>
<section id="sequence-length-scaling">
<h3>Sequence Length Scaling<a class="headerlink" href="#sequence-length-scaling" title="Link to this heading"></a></h3>
<p>Performance vs sequence length scaling:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Transformer Complexity: O(n²)
NSM Complexity: O(n) [with fixed states]

Crossover Point: ~400-500 tokens
NSM Advantage: Increases linearly with sequence length
</pre></div>
</div>
<p><strong>Empirical Scaling Laws:</strong></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\text{Transformer\_Time} \propto n^{1.97} \text{ (close to theoretical } n^2\text{)}\\\text{NSM\_Time} \propto n^{1.12} \text{ (close to linear)}\end{aligned}\end{align} \]</div>
</section>
<section id="state-count-optimization">
<h3>State Count Optimization<a class="headerlink" href="#state-count-optimization" title="Link to this heading"></a></h3>
<p>Optimal state count for different task types:</p>
<table class="docutils align-default" id="id8">
<caption><span class="caption-text">Optimal State Counts</span><a class="headerlink" href="#id8" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 30.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 30.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Task Type</p></th>
<th class="head"><p>Optimal States</p></th>
<th class="head"><p>Performance</p></th>
<th class="head"><p>Reasoning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Text Classification</p></td>
<td><p>32-64</p></td>
<td><p>89.3%</p></td>
<td><p>Simple patterns</p></td>
</tr>
<tr class="row-odd"><td><p>Language Modeling</p></td>
<td><p>64-128</p></td>
<td><p>13.6 PPL</p></td>
<td><p>Local dependencies</p></td>
</tr>
<tr class="row-even"><td><p>Long-range Tasks</p></td>
<td><p>128-256</p></td>
<td><p>94.1%</p></td>
<td><p>Complex reasoning</p></td>
</tr>
<tr class="row-odd"><td><p>Algorithmic Tasks</p></td>
<td><p>256+</p></td>
<td><p>Variable</p></td>
<td><p>Step-by-step processing</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="optimization-techniques">
<h2>Optimization Techniques<a class="headerlink" href="#optimization-techniques" title="Link to this heading"></a></h2>
<section id="model-architecture-optimizations">
<h3>Model Architecture Optimizations<a class="headerlink" href="#model-architecture-optimizations" title="Link to this heading"></a></h3>
<p><strong>1. Gradient Checkpointing</strong></p>
<p>Reduces memory usage by 40-60% with minimal speed impact:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNSM</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">num_states</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">use_checkpoint</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># Enable gradient checkpointing</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>2. Mixed Precision Training</strong></p>
<p>Improves speed by 1.5-2x with minimal accuracy loss:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">autocast</span><span class="p">,</span> <span class="n">GradScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

<span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

<span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>3. Sparse Routing</strong></p>
<p>Reduces routing computation by 50-70%:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNSM</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">num_states</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">sparse_routing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">top_k_states</span><span class="o">=</span><span class="mi">16</span>  <span class="c1"># Only route to top-16 states</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-optimizations">
<h3>Training Optimizations<a class="headerlink" href="#training-optimizations" title="Link to this heading"></a></h3>
<p><strong>1. Dynamic Batching</strong></p>
<p>Maximizes GPU utilization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">nsm.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">DynamicBatcher</span>

<span class="n">batcher</span> <span class="o">=</span> <span class="n">DynamicBatcher</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>  <span class="c1"># Maximum tokens per batch</span>
    <span class="n">max_sequences</span><span class="o">=</span><span class="mi">32</span>  <span class="c1"># Maximum sequences per batch</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>2. Learning Rate Scheduling</strong></p>
<p>Improved convergence with cosine annealing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.optim.lr_scheduler</span><span class="w"> </span><span class="kn">import</span> <span class="n">CosineAnnealingLR</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineAnnealingLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">T_max</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
    <span class="n">eta_min</span><span class="o">=</span><span class="mf">1e-6</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>3. Gradient Clipping</strong></p>
<p>Stabilizes training:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="deployment-optimizations">
<h3>Deployment Optimizations<a class="headerlink" href="#deployment-optimizations" title="Link to this heading"></a></h3>
<p><strong>1. Model Quantization</strong></p>
<p>8-bit quantization with minimal accuracy loss:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.quantization</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">quant</span>

<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quant</span><span class="o">.</span><span class="n">quantize_dynamic</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">},</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>2. ONNX Export</strong></p>
<p>Cross-platform deployment:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">dummy_input</span><span class="p">,</span>
    <span class="s2">&quot;nsm_model.onnx&quot;</span><span class="p">,</span>
    <span class="n">opset_version</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span>
    <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;input&#39;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;sequence_length&#39;</span><span class="p">}}</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>3. TensorRT Optimization</strong></p>
<p>GPU inference acceleration:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tensorrt</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">trt</span>

<span class="c1"># Convert ONNX to TensorRT</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">build_tensorrt_engine</span><span class="p">(</span>
    <span class="s2">&quot;nsm_model.onnx&quot;</span><span class="p">,</span>
    <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">fp16_mode</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="performance-monitoring">
<h2>Performance Monitoring<a class="headerlink" href="#performance-monitoring" title="Link to this heading"></a></h2>
<section id="real-time-monitoring">
<h3>Real-time Monitoring<a class="headerlink" href="#real-time-monitoring" title="Link to this heading"></a></h3>
<p>Built-in performance monitoring tools:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">nsm.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">PerformanceMonitor</span>

<span class="n">monitor</span> <span class="o">=</span> <span class="n">PerformanceMonitor</span><span class="p">()</span>

<span class="c1"># Training loop with monitoring</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">monitor</span><span class="o">.</span><span class="n">epoch_context</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">monitor</span><span class="o">.</span><span class="n">batch_context</span><span class="p">():</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Get performance statistics</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">monitor</span><span class="o">.</span><span class="n">get_epoch_stats</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Memory: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;memory_peak&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">MB, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;Speed: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;tokens_per_second&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> tok/s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="profiling-tools">
<h3>Profiling Tools<a class="headerlink" href="#profiling-tools" title="Link to this heading"></a></h3>
<p><strong>1. PyTorch Profiler Integration</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span>
    <span class="n">activities</span><span class="o">=</span><span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

<span class="c1"># Export trace for analysis</span>
<span class="n">prof</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="p">(</span><span class="s2">&quot;nsm_trace.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>2. Custom Timing Analysis</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">nsm.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">ComponentTimer</span>

<span class="n">timer</span> <span class="o">=</span> <span class="n">ComponentTimer</span><span class="p">()</span>

<span class="k">with</span> <span class="n">timer</span><span class="o">.</span><span class="n">time</span><span class="p">(</span><span class="s2">&quot;routing&quot;</span><span class="p">):</span>
    <span class="n">routed_states</span> <span class="o">=</span> <span class="n">router</span><span class="p">(</span><span class="n">input_embeddings</span><span class="p">)</span>

<span class="k">with</span> <span class="n">timer</span><span class="o">.</span><span class="n">time</span><span class="p">(</span><span class="s2">&quot;propagation&quot;</span><span class="p">):</span>
    <span class="n">updated_states</span> <span class="o">=</span> <span class="n">propagator</span><span class="p">(</span><span class="n">routed_states</span><span class="p">)</span>

<span class="c1"># Print timing breakdown</span>
<span class="n">timer</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="benchmarking-guidelines">
<h2>Benchmarking Guidelines<a class="headerlink" href="#benchmarking-guidelines" title="Link to this heading"></a></h2>
<section id="reproducible-benchmarks">
<h3>Reproducible Benchmarks<a class="headerlink" href="#reproducible-benchmarks" title="Link to this heading"></a></h3>
<p>For fair comparison with other models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

<span class="c1"># Set random seeds</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Deterministic operations</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</section>
<section id="standard-hardware-configurations">
<h3>Standard Hardware Configurations<a class="headerlink" href="#standard-hardware-configurations" title="Link to this heading"></a></h3>
<p><strong>Recommended Testing Configurations:</strong></p>
<ul class="simple">
<li><p><strong>Development</strong>: Single V100 32GB</p></li>
<li><p><strong>Production</strong>: 4x A100 80GB</p></li>
<li><p><strong>Mobile</strong>: CPU-only with quantization</p></li>
<li><p><strong>Edge</strong>: Single T4 or similar</p></li>
</ul>
<p><strong>Memory Benchmarks:</strong></p>
<ul class="simple">
<li><p>Test with 512, 1024, 2048, 4096, 8192 token sequences</p></li>
<li><p>Measure peak memory usage and training throughput</p></li>
<li><p>Compare against baseline Transformer implementation</p></li>
</ul>
<p><strong>Speed Benchmarks:</strong></p>
<ul class="simple">
<li><p>Measure tokens/second for training and inference</p></li>
<li><p>Test both single-sequence and batched inference</p></li>
<li><p>Include model loading and initialization time</p></li>
</ul>
</section>
</section>
<section id="performance-best-practices">
<h2>Performance Best Practices<a class="headerlink" href="#performance-best-practices" title="Link to this heading"></a></h2>
<section id="model-configuration">
<h3>Model Configuration<a class="headerlink" href="#model-configuration" title="Link to this heading"></a></h3>
<p><strong>1. Choose Appropriate State Count</strong></p>
<ul class="simple">
<li><p>Start with 64 states for most tasks</p></li>
<li><p>Increase to 128-256 for complex reasoning</p></li>
<li><p>Monitor performance/memory trade-off</p></li>
</ul>
<p><strong>2. Optimize Hyperparameters</strong></p>
<ul class="simple">
<li><p>Learning rate: 1e-4 to 5e-4 (typically 2x higher than Transformers)</p></li>
<li><p>Batch size: Maximize within memory constraints</p></li>
<li><p>Gradient accumulation: Use for effective large batch sizes</p></li>
</ul>
<p><strong>3. Architecture Choices</strong></p>
<ul class="simple">
<li><p>Use hybrid models for complex tasks requiring both efficiency and capacity</p></li>
<li><p>Enable sparse routing for very large models</p></li>
<li><p>Consider memory-augmented variants for algorithmic tasks</p></li>
</ul>
</section>
<section id="training-strategy">
<h3>Training Strategy<a class="headerlink" href="#training-strategy" title="Link to this heading"></a></h3>
<p><strong>1. Progressive Training</strong></p>
<ul class="simple">
<li><p>Start with shorter sequences, gradually increase length</p></li>
<li><p>Use curriculum learning for complex tasks</p></li>
<li><p>Fine-tune from general-purpose models when possible</p></li>
</ul>
<p><strong>2. Monitoring and Debugging</strong></p>
<ul class="simple">
<li><p>Track state utilization and routing patterns</p></li>
<li><p>Monitor gradient norms and training stability</p></li>
<li><p>Use visualization tools to understand model behavior</p></li>
</ul>
<p><strong>3. Regularization</strong></p>
<ul class="simple">
<li><p>Dropout: 0.1-0.3 (similar to Transformers)</p></li>
<li><p>Weight decay: 0.01-0.1</p></li>
<li><p>Label smoothing: 0.1 for classification tasks</p></li>
</ul>
<p>This comprehensive performance analysis demonstrates that NSMs provide significant computational advantages while maintaining or improving accuracy across diverse tasks. The linear scaling properties make them particularly attractive for production deployments with long sequences or resource constraints.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="design.html" class="btn btn-neutral float-left" title="Architecture Design" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Rei Calasso.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>