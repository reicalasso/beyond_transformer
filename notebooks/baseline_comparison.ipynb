{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Baseline Experiments\n",
    "\n",
    "This notebook compares Neural State Machines (NSM) against various baseline architectures:\n",
    "- Transformers\n",
    "- LSTM/GRU\n",
    "- RWKV (not implemented here, conceptual comparison)\n",
    "- S4 (not implemented here, conceptual comparison)\n",
    "\n",
    "## Metrics\n",
    "- Accuracy\n",
    "- F1 Score\n",
    "- Memory Usage\n",
    "- FLOPs (conceptual)\n",
    "- Training Speed\n",
    "\n",
    "## Datasets\n",
    "- MNIST\n",
    "- Tiny Shakespeare\n",
    "- IMDb\n",
    "- CIFAR-10\n",
    "- LRA (Long Range Arena - conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "# Import our components\n",
    "from nsm import NSMLayer, TokenToStateRouter, StateManager\n",
    "from nsm.models import SimpleNSM\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"Simple LSTM baseline model.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.0):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Take the last output\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    \"\"\"Simple GRU baseline model.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.0):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, \n",
    "                         batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # GRU forward pass\n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        # Take the last output\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"Simple Transformer baseline model.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim, dropout=0.0):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        # Input embedding\n",
    "        self.input_embedding = nn.Linear(input_dim, model_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1000, model_dim))\n",
    "        \n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(model_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim]\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Embed input\n",
    "        x = self.input_embedding(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pos_enc = self.pos_encoding[:seq_len, :].unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        x = x + pos_enc\n",
    "        \n",
    "        # Apply transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Output\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleNSMModel(nn.Module):\n",
    "    \"\"\"Simple NSM model for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, state_dim, num_states, output_dim):\n",
    "        super(SimpleNSMModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.num_states = num_states\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Input embedding\n",
    "        self.input_embedding = nn.Linear(input_dim, state_dim)\n",
    "        \n",
    "        # NSM layer\n",
    "        self.nsm_layer = NSMLayer(state_dim=state_dim, token_dim=state_dim, num_heads=4)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(state_dim * num_states, output_dim)\n",
    "        \n",
    "        # Initialize states\n",
    "        self.initial_states = nn.Parameter(torch.randn(1, num_states, state_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim]\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Embed input\n",
    "        x = self.input_embedding(x)  // [batch_size, seq_len, state_dim]\n",
    "        \n",
    "        // Initialize states\n",
    "        states = self.initial_states.repeat(batch_size, 1, 1)  // [batch_size, num_states, state_dim]\n",
    "        \n",
    "        // Process sequence\n",
    "        for t in range(seq_len):\n",
    "            // Get input at time t\n",
    "            input_t = x[:, t, :].unsqueeze(1).repeat(1, self.num_states, 1)  // [batch_size, num_states, state_dim]\n",
    "            \n",
    "            // Update states\n",
    "            states = self.nsm_layer(states, input_t)\n",
    "        \n",
    "        // Global pooling\n",
    "        pooled_states = states.view(batch_size, -1)  // [batch_size, state_dim * num_states]\n",
    "        \n",
    "        // Output projection\n",
    "        output = self.output_projection(pooled_states)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mnist_dataset(num_samples=1000):\n",
    "    \"\"\"Create MNIST-like dataset.\"\"\"\n",
    "    // MNIST: 28x28 = 784 pixels, 10 classes\n",
    "    X = torch.randn(num_samples, 784)\n",
    "    y = torch.randint(0, 10, (num_samples,))\n",
    "    return torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "def create_tiny_shakespeare_dataset(num_samples=1000, seq_len=256):\n",
    "    \"\"\"Create Tiny Shakespeare-like dataset.\"\"\"\n",
    "    // Vocabulary size for characters (simplified)\n",
    "    vocab_size = 100\n",
    "    X = torch.randint(0, vocab_size, (num_samples, seq_len))\n",
    "    // For language modeling, predict next character\n",
    "    y = X[:, -1]  // Just predict last character for simplicity\n",
    "    return torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "def create_imdb_dataset(num_samples=1000, seq_len=512):\n",
    "    \"\"\"Create IMDb-like dataset.\"\"\"\n",
    "    // Vocabulary size for words (simplified)\n",
    "    vocab_size = 10000\n",
    "    X = torch.randint(0, vocab_size, (num_samples, seq_len))\n",
    "    // Binary sentiment classification\n",
    "    y = torch.randint(0, 2, (num_samples,))\n",
    "    return torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "def create_cifar10_dataset(num_samples=1000):\n",
    "    \"\"\"Create CIFAR-10-like dataset.\"\"\"\n",
    "    // CIFAR-10: 32x32x3 = 3072 pixels, 10 classes\n",
    "    X = torch.randn(num_samples, 3072)\n",
    "    y = torch.randint(0, 10, (num_samples,))\n",
    "    return torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "def create_lra_dataset(num_samples=1000, seq_len=1024):\n",
    "    \"\"\"Create LRA-like dataset (conceptual).\"\"\"\n",
    "    // Long sequence classification\n",
    "    X = torch.randn(num_samples, seq_len, 1)  // Single feature, long sequence\n",
    "    y = torch.randint(0, 2, (num_samples,))\n",
    "    return torch.utils.data.TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, epochs=5, lr=0.001, weight_decay=1e-5):\n",
    "    \"\"\"Train model and return metrics.\"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    // Memory tracking\n",
    "    process = psutil.Process(os.getpid())\n",
    "    initial_memory = process.memory_info().rss / 1024 / 1024  // MB\n",
    "    \n",
    "    // Training time tracking\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            // Handle discrete data (embedding)\n",
    "            if data.dtype == torch.int64:\n",
    "                // For text data, we need embedding\n",
    "                if hasattr(model, 'embedding'):\n",
    "                    data = model.embedding(data)\n",
    "                else:\n",
    "                    // Add embedding to models that need it\n",
    "                    embedding = nn.Embedding(10000, 128).to(device)\n",
    "                    data = embedding(data)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            // Limit batches for quick testing\n",
    "            if batch_idx > 20:\n",
    "                break\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        epoch_losses.append(avg_loss)\n",
    "        epoch_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Loss={avg_loss:.4f}, Accuracy={accuracy:.2f}%\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    final_memory = process.memory_info().rss / 1024 / 1024  // MB\n",
    "    \n",
    "    // Calculate metrics\n",
    "    training_time = end_time - start_time\n",
    "    memory_usage = final_memory - initial_memory\n",
    "    final_accuracy = epoch_accuracies[-1]\n",
    "    \n",
    "    return {\n",
    "        'accuracy': final_accuracy,\n",
    "        'losses': epoch_losses,\n",
    "        'accuracies': epoch_accuracies,\n",
    "        'memory_usage': memory_usage,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            // Handle discrete data (embedding)\n",
    "            if data.dtype == torch.int64:\n",
    "                if hasattr(model, 'embedding'):\n",
    "                    data = model.embedding(data)\n",
    "                else:\n",
    "                    // Add embedding to models that need it\n",
    "                    embedding = nn.Embedding(10000, 128).to(device)\n",
    "                    data = embedding(data)\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            // Limit for quick testing\n",
    "            if total > 200:\n",
    "                break\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Factory Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_type, dataset_name, input_dim, output_dim):\n",
    "    \"\"\"Create model based on type and dataset.\"\"\"\n",
    "    \n",
    "    if model_type == \"LSTM\":\n",
    "        return SimpleLSTM(input_dim=input_dim, hidden_dim=128, num_layers=2, output_dim=output_dim)\n",
    "    \n",
    "    elif model_type == \"GRU\":\n",
    "        return SimpleGRU(input_dim=input_dim, hidden_dim=128, num_layers=2, output_dim=output_dim)\n",
    "    \n",
    "    elif model_type == \"Transformer\":\n",
    "        return SimpleTransformer(input_dim=input_dim, model_dim=128, num_heads=4, \n",
    "                               num_layers=2, output_dim=output_dim)\n",
    "    \n",
    "    elif model_type == \"NSM\":\n",
    "        return SimpleNSMModel(input_dim=input_dim, state_dim=64, num_states=16, output_dim=output_dim)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Comparison Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline_comparison():\n",
    "    \"\"\"Run baseline comparison experiment.\"\"\"\n",
    "    \n",
    "    // Model types to compare\n",
    "    model_types = [\"LSTM\", \"GRU\", \"Transformer\", \"NSM\"]\n",
    "    \n",
    "    // Dataset configurations\n",
    "    datasets = {\n",
    "        'MNIST': {\n",
    "            'create_func': create_mnist_dataset,\n",
    "            'input_dim': 784,\n",
    "            'output_dim': 10,\n",
    "            'num_samples': 1000\n",
    "        },\n",
    "        'Tiny_Shakespeare': {\n",
    "            'create_func': create_tiny_shakespeare_dataset,\n",
    "            'input_dim': 100,  // vocab_size\n",
    "            'output_dim': 100,  // vocab_size\n",
    "            'num_samples': 500\n",
    "        },\n",
    "        'IMDb': {\n",
    "            'create_func': create_imdb_dataset,\n",
    "            'input_dim': 10000,  // vocab_size\n",
    "            'output_dim': 2,     // binary classification\n",
    "            'num_samples': 500\n",
    "        },\n",
    "        'CIFAR10': {\n",
    "            'create_func': create_cifar10_dataset,\n",
    "            'input_dim': 3072,\n",
    "            'output_dim': 10,\n",
    "            'num_samples': 1000\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Results storage\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    // Experiment parameters\n",
    "    epochs = 3\n",
    "    batch_size = 32\n",
    "    \n",
    "    for dataset_name, dataset_config in datasets.items():\n",
    "        print(f\"\\n=== Testing {dataset_name} ===\")\n",
    "        \n",
    "        // Create dataset\n",
    "        train_dataset = dataset_config['create_func'](num_samples=dataset_config['num_samples'])\n",
    "        test_dataset = dataset_config['create_func'](num_samples=200)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for model_type in model_types:\n",
    "            print(f\"  Testing {model_type}...\")\n",
    "            \n",
    "            try:\n",
    "                // Create model\n",
    "                model = create_model(\n",
    "                    model_type=model_type,\n",
    "                    dataset_name=dataset_name,\n",
    "                    input_dim=dataset_config['input_dim'],\n",
    "                    output_dim=dataset_config['output_dim']\n",
    "                )\n",
    "                \n",
    "                // Train model\n",
    "                metrics = train_model(model, train_loader, epochs=epochs)\n",
    "                \n",
    "                // Evaluate model\n",
    "                test_accuracy = evaluate_model(model, test_loader)\n",
    "                \n",
    "                // Store results\n",
    "                results[dataset_name][model_type] = {\n",
    "                    'train_accuracy': metrics['accuracy'],\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                    'memory_usage': metrics['memory_usage'],\n",
    "                    'training_time': metrics['training_time'],\n",
    "                    'losses': metrics['losses'],\n",
    "                    'accuracies': metrics['accuracies']\n",
    "                }\n",
    "                \n",
    "                print(f\"    Train Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "                print(f\"    Test Accuracy: {test_accuracy:.2f}%\")\n",
    "                print(f\"    Memory Usage: {metrics['memory_usage']:.2f} MB\")\n",
    "                print(f\"    Training Time: {metrics['training_time']:.2f} seconds\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error with {model_type}: {e}\")\n",
    "                results[dataset_name][model_type] = {\n",
    "                    'train_accuracy': 0.0,\n",
    "                    'test_accuracy': 0.0,\n",
    "                    'memory_usage': 0.0,\n",
    "                    'training_time': 0.0,\n",
    "                    'losses': [],\n",
    "                    'accuracies': [],\n",
    "                    'error': str(e)\n",
    "                }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment and Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Run the baseline comparison\n",
    "results = run_baseline_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_results(results):\n",
    "    \"\"\"Plot comparison results.\"\"\"\n",
    "    \n",
    "    model_types = [\"LSTM\", \"GRU\", \"Transformer\", \"NSM\"]\n",
    "    datasets = list(results.keys())\n",
    "    \n",
    "    // Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Baseline Model Comparison Results', fontsize=16)\n",
    "    \n",
    "    // Plot 1: Test Accuracy\n",
    "    ax = axes[0, 0]\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, model_type in enumerate(model_types):\n",
    "        accuracies = [results[dataset][model_type].get('test_accuracy', 0) for dataset in datasets]\n",
    "        ax.bar(x + i*width, accuracies, width, label=model_type)\n",
    "    \n",
    "    ax.set_xlabel('Datasets')\n",
    "    ax.set_ylabel('Test Accuracy (%)')\n",
    "    ax.set_title('Test Accuracy by Model and Dataset')\n",
    "    ax.set_xticks(x + width * 1.5)\n",
    "    ax.set_xticklabels(datasets)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    // Plot 2: Training Time\n",
    "    ax = axes[0, 1]\n",
    "    for i, model_type in enumerate(model_types):\n",
    "        times = [results[dataset][model_type].get('training_time', 0) for dataset in datasets]\n",
    "        ax.bar(x + i*width, times, width, label=model_type)\n",
    "    \n",
    "    ax.set_xlabel('Datasets')\n",
    "    ax.set_ylabel('Training Time (seconds)')\n",
    "    ax.set_title('Training Time by Model and Dataset')\n",
    "    ax.set_xticks(x + width * 1.5)\n",
    "    ax.set_xticklabels(datasets)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    // Plot 3: Memory Usage\n",
    "    ax = axes[1, 0]\n",
    "    for i, model_type in enumerate(model_types):\n",
    "        memory = [results[dataset][model_type].get('memory_usage', 0) for dataset in datasets]\n",
    "        ax.bar(x + i*width, memory, width, label=model_type)\n",
    "    \n",
    "    ax.set_xlabel('Datasets')\n",
    "    ax.set_ylabel('Memory Usage (MB)')\n",
    "    ax.set_title('Memory Usage by Model and Dataset')\n",
    "    ax.set_xticks(x + width * 1.5)\n",
    "    ax.set_xticklabels(datasets)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    // Plot 4: Training Curves (example for first dataset)\n",
    "    ax = axes[1, 1]\n",
    "    if datasets:\n",
    "        first_dataset = datasets[0]\n",
    "        for model_type in model_types:\n",
    "            accuracies = results[first_dataset][model_type].get('accuracies', [])\n",
    "            if accuracies:\n",
    "                ax.plot(accuracies, marker='o', label=model_type)\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Training Accuracy (%)')\n",
    "        ax.set_title(f'Training Curves - {first_dataset}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Plot the results\n",
    "plot_comparison_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, filename='baseline_comparison_results.json'):\n",
    "    \"\"\"Save results to JSON file.\"\"\"\n",
    "    // Convert results to JSON-serializable format\n",
    "    serializable_results = {}\n",
    "    for dataset_name, models in results.items():\n",
    "        serializable_results[dataset_name] = {}\n",
    "        for model_type, metrics in models.items():\n",
    "            serializable_results[dataset_name][model_type] = {}\n",
    "            for key, value in metrics.items():\n",
    "                if isinstance(value, (int, float, str, bool)):\n",
    "                    serializable_results[dataset_name][model_type][key] = value\n",
    "                elif isinstance(value, list):\n",
    "                    serializable_results[dataset_name][model_type][key] = [float(x) if isinstance(x, (int, float)) else str(x) for x in value]\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Save results\n",
    "save_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive comparison of Neural State Machines against traditional architectures:\n",
    "\n",
    "### Models Compared\n",
    "1. **LSTM**: Traditional recurrent architecture\n",
    "2. **GRU**: Simplified recurrent architecture\n",
    "3. **Transformer**: Self-attention based architecture\n",
    "4. **NSM**: Neural State Machine (our implementation)\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy**: Training and test performance\n",
    "- **Memory Usage**: RAM consumption during training\n",
    "- **Training Time**: Wall-clock time for training\n",
    "- **Training Curves**: Convergence behavior\n",
    "\n",
    "### Datasets\n",
    "- **MNIST**: Image classification\n",
    "- **Tiny Shakespeare**: Text generation\n",
    "- **IMDb**: Sentiment classification\n",
    "- **CIFAR-10**: Image classification\n",
    "\n",
    "The results provide insights into the relative strengths and weaknesses of each architecture across different tasks and data types."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}