{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Neural State Machines\n",
    "\n",
    "This notebook provides a quick introduction to Neural State Machines (NSM) and how to use them for sequence processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, let's make sure we have the required packages installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "Let's start with the core components of NSM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import NSM components\n",
    "from nsm import StatePropagator\n",
    "\n",
    "print(\"✅ Neural State Machine components imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. State Propagation\n",
    "\n",
    "The core of NSM is the `StatePropagator` which handles how states are updated, retained, or reset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a state propagator\n",
    "state_dim = 64\n",
    "propagator = StatePropagator(\n",
    "    state_dim=state_dim,\n",
    "    gate_type='gru',  # or 'lstm'\n",
    "    enable_communication=False  # Disable for now\n",
    ")\n",
    "\n",
    "print(f\"State propagator created with state dimension: {state_dim}\")\n",
    "print(f\"Gate type: {'GRU' if propagator.gate_type == 'gru' else 'LSTM'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single state update example\n",
    "batch_size = 4\n",
    "prev_state = torch.randn(batch_size, state_dim)\n",
    "new_input = torch.randn(batch_size, state_dim)\n",
    "\n",
    "print(f\"Previous state shape: {prev_state.shape}\")\n",
    "print(f\"New input shape: {new_input.shape}\")\n",
    "\n",
    "# Apply state update\n",
    "updated_state = propagator(prev_state, new_input)\n",
    "\n",
    "print(f\"Updated state shape: {updated_state.shape}\")\n",
    "\n",
    "# Analyze the update\n",
    "state_change = torch.mean(torch.abs(updated_state - prev_state))\n",
    "print(f\"Average state change: {state_change.item():.4f}\")\n",
    "print(f\"Previous state norm: {torch.mean(torch.norm(prev_state, dim=1)):.4f}\")\n",
    "print(f\"Updated state norm: {torch.mean(torch.norm(updated_state, dim=1)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-State Processing\n",
    "\n",
    "NSM can process multiple states simultaneously with communication between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-state propagator with communication\n",
    "num_states = 8\n",
    "multi_propagator = StatePropagator(\n",
    "    state_dim=state_dim,\n",
    "    gate_type='gru',\n",
    "    num_heads=4,\n",
    "    enable_communication=True  # Enable state-to-state communication\n",
    ")\n",
    "\n",
    "print(f\"Multi-state propagator created with {num_states} states\")\n",
    "print(f\"Communication enabled: {multi_propagator.enable_communication}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-state update example\n",
    "prev_states = torch.randn(batch_size, num_states, state_dim)\n",
    "new_inputs = torch.randn(batch_size, num_states, state_dim)\n",
    "\n",
    "print(f\"Previous states shape: {prev_states.shape}\")\n",
    "print(f\"New inputs shape: {new_inputs.shape}\")\n",
    "\n",
    "# Apply multi-state update\n",
    "updated_states = multi_propagator(prev_states, new_inputs)\n",
    "\n",
    "print(f\"Updated states shape: {updated_states.shape}\")\n",
    "\n",
    "# Analyze the multi-state update\n",
    "state_changes = torch.mean(torch.abs(updated_states - prev_states), dim=(1, 2))\n",
    "print(f\"Average state change per batch: {torch.mean(state_changes).item():.4f}\")\n",
    "print(f\"Max state change per batch: {torch.max(state_changes).item():.4f}\")\n",
    "print(f\"Min state change per batch: {torch.min(state_changes).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complete NSM Model\n",
    "\n",
    "Let's create a complete NSM model for a simple classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nsm.models import SimpleNSM\n",
    "\n",
    "# Create a simple NSM model for MNIST-like classification\n",
    "model = SimpleNSM(\n",
    "    input_dim=784,      # Flattened 28x28 images\n",
    "    state_dim=128,      # State vector dimension\n",
    "    num_states=16,      # Number of state nodes\n",
    "    output_dim=10,      # 10-class classification\n",
    "    gate_type='gru'     # Gating mechanism\n",
    ")\n",
    "\n",
    "print(\"✅ Simple NSM model created!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with sample data\n",
    "sample_batch_size = 8\n",
    "x = torch.randn(sample_batch_size, 784)  # Random input (like flattened MNIST)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output range: [{output.min().item():.3f}, {output.max().item():.3f}]\")\n",
    "print(f\"Output probabilities sum (first sample): {torch.softmax(output[0], dim=0).sum().item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Example\n",
    "\n",
    "Let's train our NSM model on a simple synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic classification data\n",
    "X, y = make_classification(n_samples=1000, n_features=784, n_classes=10, \n",
    "                           n_informative=200, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Feature dimension: {X_train.shape[1]}\")\n",
    "print(f\"Number of classes: {len(torch.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total += target.size(0)\n",
    "        \n",
    "        # Limit training for demo\n",
    "        if batch_idx > 10:  # Just for demonstration\n",
    "            break\n",
    "    \n",
    "    avg_loss = total_loss / (batch_idx + 1)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            # Limit evaluation for demo\n",
    "            if batch_idx > 5:  # Just for demonstration\n",
    "                break\n",
    "    \n",
    "    avg_loss = total_loss / (batch_idx + 1)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training demo\n",
    "epochs = 5\n",
    "print(\"Starting training demo...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}:')\n",
    "    print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    print()\n",
    "\n",
    "print(\"🎉 Training demo completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization and Analysis\n",
    "\n",
    "Let's visualize some aspects of our NSM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model outputs\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_input = X_test[:4].to(device)  # First 4 test samples\n",
    "    sample_output = model(sample_input)\n",
    "    sample_probs = torch.softmax(sample_output, dim=1)\n",
    "\n",
    "print(\"Sample predictions:\")\n",
    "for i in range(4):\n",
    "    predicted_class = sample_probs[i].argmax().item()\n",
    "    confidence = sample_probs[i].max().item()\n",
    "    actual_class = y_test[i].item()\n",
    "    print(f\"  Sample {i+1}: Predicted={predicted_class} (confidence={confidence:.3f}), Actual={actual_class}, \"\n",
    "          f\"Correct={'✅' if predicted_class == actual_class else '❌'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probability distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(4):\n",
    "    axes[i].bar(range(10), sample_probs[i].cpu().numpy())\n",
    "    axes[i].set_xlabel('Class')\n",
    "    axes[i].set_ylabel('Probability')\n",
    "    axes[i].set_title(f'Sample {i+1} - Actual Class: {y_test[i].item()}')\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Features\n",
    "\n",
    "NSM also supports advanced features like dynamic state management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nsm import StateManager\n",
    "\n",
    "# Create state manager with dynamic allocation\n",
    "state_manager = StateManager(\n",
    "    state_dim=128,\n",
    "    max_states=32,\n",
    "    initial_states=16,\n",
    "    prune_threshold=0.3\n",
    ")\n",
    "\n",
    "print(\"✅ State manager created with dynamic allocation!\")\n",
    "print(f\"Initial active states: {state_manager.get_active_count()}\")\n",
    "\n",
    "# Get current states\n",
    "states = state_manager()\n",
    "print(f\"Current states shape: {states.shape}\")\n",
    "\n",
    "# Get importance scores\n",
    "importance_scores = state_manager.get_importance_scores()\n",
    "active_count = state_manager.get_active_count()\n",
    "print(f\"Average importance score: {torch.mean(importance_scores[:active_count]).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate dynamic state management\n",
    "print(\"\\nDynamic state management demo:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for step in range(5):\n",
    "    # Simulate some processing that affects state importance\n",
    "    # (In real training, this would happen through backpropagation)\n",
    "    \n",
    "    current_count = state_manager.get_active_count()\n",
    "    print(f\"Step {step+1}: Active states = {current_count}\")\n",
    "    \n",
    "    # Periodically manage states\n",
    "    if step % 2 == 0 and step > 0:\n",
    "        pruned = state_manager.prune_low_importance_states()\n",
    "        allocated = state_manager.allocate_states(2)\n",
    "        print(f\"  Pruned: {pruned}, Allocated: {allocated}\")\n",
    "        print(f\"  New active count: {state_manager.get_active_count()}\")\n",
    "\n",
    "final_count = state_manager.get_active_count()\n",
    "print(f\"\\nFinal active states: {final_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "✅ **Basic state propagation** with gated updates\n",
    "✅ **Multi-state processing** with communication\n",
    "✅ **Complete NSM models** for classification tasks\n",
    "✅ **Training workflows** with optimization\n",
    "✅ **Visualization and analysis** of model behavior\n",
    "✅ **Advanced features** like dynamic state management\n",
    "\n",
    "Neural State Machines provide a powerful alternative to traditional Transformers with:\n",
    "\n",
    "- **Efficient computation**: Linear scaling with sequence length\n",
    "- **Interpretability**: Explicit state management and tracking\n",
    "- **Flexibility**: Dynamic state allocation and pruning\n",
    "- **Performance**: Competitive results with reduced resource usage\n",
    "\n",
    "For more advanced usage and detailed documentation, please refer to:\n",
    "- [Architecture Overview](../docs/architecture/architecture_overview.md)\n",
    "- [API Reference](../docs/api/api_reference.md)\n",
    "- [Full Tutorial](../docs/tutorials/tutorial.md)\n",
    "\n",
    "Happy experimenting with Neural State Machines! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}