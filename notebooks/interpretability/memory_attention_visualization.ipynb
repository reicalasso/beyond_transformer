{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory and Attention Visualization for Neural State Machines\n",
    "\n",
    "This notebook focuses specifically on visualizing attention maps and external memory contents in Neural State Machine models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path().cwd().parent.parent))\n",
    "\n",
    "# Import NSM components and visualization tools\n",
    "from nsm.utils.advanced_visualizer import AdvancedNSMVisualizer\n",
    "from nsm.modules.ntm_memory import NTMMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attention Map Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced visualizer\n",
    "visualizer = AdvancedNSMVisualizer(figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate different types of attention patterns\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Self-attention pattern\n",
    "self_attention = torch.softmax(torch.randn(8, 8), dim=-1)\n",
    "\n",
    "# 2. Token-to-state attention\n",
    "token_state_attention = torch.softmax(torch.randn(12, 6), dim=-1)  # 12 tokens, 6 states\n",
    "\n",
    "# 3. State-to-state attention\n",
    "state_state_attention = torch.softmax(torch.randn(6, 6), dim=-1)  # 6 states\n",
    "\n",
    "# 4. Multi-head attention\n",
    "multi_head_attention = torch.softmax(torch.randn(4, 8, 8), dim=-1)  # 4 heads, 8x8\n",
    "\n",
    "print(\"Attention patterns generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize self-attention\n",
    "fig1 = visualizer.plot_attention_map(\n",
    "    self_attention,\n",
    "    title=\"Self-Attention Pattern\",\n",
    "    x_labels=[f\"Pos{i}\" for i in range(8)],\n",
    "    y_labels=[f\"Query{i}\" for i in range(8)]\n",
    ")\n",
    "\n",
    "print(\"Self-attention visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize token-to-state attention\n",
    "fig2 = visualizer.plot_token_to_state_routing(\n",
    "    token_state_attention,\n",
    "    token_labels=[f\"T{i}\" for i in range(12)],\n",
    "    state_labels=[f\"S{i}\" for i in range(6)],\n",
    "    title=\"Token-to-State Attention Routing\"\n",
    ")\n",
    "\n",
    "print(\"Token-to-state attention visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize state-to-state attention\n",
    "fig3 = visualizer.plot_state_communication(\n",
    "    state_state_attention,\n",
    "    state_labels=[f\"State{i}\" for i in range(6)],\n",
    "    title=\"State-to-State Communication Pattern\"\n",
    ")\n",
    "\n",
    "print(\"State-to-state attention visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-head attention patterns\n",
    "fig4 = visualizer.plot_attention_patterns(\n",
    "    multi_head_attention,\n",
    "    pattern_names=[f\"Head {i+1}\" for i in range(4)],\n",
    "    title=\"Multi-Head Attention Patterns\"\n",
    ")\n",
    "\n",
    "print(\"Multi-head attention visualization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. External Memory Content Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NTM memory for demonstration\n",
    "ntm_memory = NTMMemory(mem_size=32, mem_dim=16, num_read_heads=2, num_write_heads=1)\n",
    "\n",
    "# Initialize with some values\n",
    "memory_content = ntm_memory.get_memory_state()\n",
    "\n",
    "print(f\"NTM Memory shape: {memory_content.shape}\")\n",
    "print(f\"Memory content range: [{memory_content.min():.3f}, {memory_content.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize memory content\n",
    "fig5 = visualizer.plot_memory_content(\n",
    "    memory_content,\n",
    "    title=\"NTM External Memory Content\"\n",
    ")\n",
    "\n",
    "print(\"Memory content visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate memory evolution\n",
    "def simulate_memory_evolution(ntm, steps=5):\n",
    "    \"\"\"Simulate memory evolution over multiple steps.\"\"\"\n",
    "    memory_states = []\n",
    "    \n",
    "    # Get initial memory\n",
    "    initial_memory = ntm.get_memory_state()\n",
    "    memory_states.append(initial_memory)\n",
    "    \n",
    "    # Simulate memory operations\n",
    "    for step in range(steps):\n",
    "        # Generate random parameters for memory operations\n",
    "        batch_size = 1\n",
    "        read_keys = torch.randn(batch_size, ntm.num_read_heads, ntm.mem_dim)\n",
    "        write_keys = torch.randn(batch_size, ntm.num_write_heads, ntm.mem_dim)\n",
    "        read_strengths = torch.randn(batch_size, ntm.num_read_heads)\n",
    "        write_strengths = torch.randn(batch_size, ntm.num_write_heads)\n",
    "        erase_vectors = torch.sigmoid(torch.randn(batch_size, ntm.num_write_heads, ntm.mem_dim))\n",
    "        add_vectors = torch.tanh(torch.randn(batch_size, ntm.num_write_heads, ntm.mem_dim))\n",
    "        \n",
    "        # Perform memory operations\n",
    "        read_vectors, updated_memory = ntm(\n",
    "            read_keys, write_keys, read_strengths, write_strengths,\n",
    "            erase_vectors, add_vectors\n",
    "        )\n",
    "        \n",
    "        # Store memory state\n",
    "        memory_states.append(updated_memory)\n",
    "    \n",
    "    return memory_states\n",
    "\n",
    "# Simulate memory evolution\n",
    "memory_evolution = simulate_memory_evolution(ntm_memory, steps=4)\n",
    "print(f\"Memory evolution steps: {len(memory_evolution)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize memory evolution\n",
    "fig6 = visualizer.plot_state_evolution(\n",
    "    memory_evolution,\n",
    "    title=\"Memory Content Evolution Over Time\"\n",
    ")\n",
    "\n",
    "print(\"Memory evolution visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize memory slot importance\n",
    "importance_scores = ntm_memory.get_importance_scores()\n",
    "\n",
    "fig7 = visualizer.plot_memory_importance(\n",
    "    importance_scores,\n",
    "    title=\"Memory Slot Importance Scores\"\n",
    ")\n",
    "\n",
    "print(\"Memory importance visualization complete.\")\n",
    "print(f\"Average importance score: {importance_scores.mean().item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory Read/Write Operations Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate read/write operations\n",
    "def simulate_read_write_operations(ntm):\n",
    "    \"\"\"Simulate read and write operations to visualize attention weights.\"\"\"\n",
    "    batch_size = 1\n",
    "    \n",
    "    # Generate parameters for memory operations\n",
    "    read_keys = torch.randn(batch_size, ntm.num_read_heads, ntm.mem_dim)\n",
    "    write_keys = torch.randn(batch_size, ntm.num_write_heads, ntm.mem_dim)\n",
    "    read_strengths = torch.randn(batch_size, ntm.num_read_heads)\n",
    "    write_strengths = torch.randn(batch_size, ntm.num_write_heads)\n",
    "    erase_vectors = torch.sigmoid(torch.randn(batch_size, ntm.num_write_heads, ntm.mem_dim))\n",
    "    add_vectors = torch.tanh(torch.randn(batch_size, ntm.num_write_heads, ntm.mem_dim))\n",
    "    \n",
    "    # Get initial read/write weights\n",
    "    initial_read_weights = ntm.get_read_weights()\n",
    "    initial_write_weights = ntm.get_write_weights()\n",
    "    \n",
    "    # Perform memory operations\n",
    "    read_vectors, updated_memory = ntm(\n",
    "        read_keys, write_keys, read_strengths, write_strengths,\n",
    "        erase_vectors, add_vectors\n",
    "    )\n",
    "    \n",
    "    # Get updated weights\n",
    "    final_read_weights = ntm.get_read_weights()\n",
    "    final_write_weights = ntm.get_write_weights()\n",
    "    \n",
    "    return {\n",
    "        'initial_read': initial_read_weights,\n",
    "        'initial_write': initial_write_weights,\n",
    "        'final_read': final_read_weights,\n",
    "        'final_write': final_write_weights,\n",
    "        'read_vectors': read_vectors,\n",
    "        'memory': updated_memory\n",
    "}\n",
    "\n",
    "# Simulate operations\n",
    "operation_results = simulate_read_write_operations(ntm_memory)\n",
    "print(\"Memory operations simulated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize read/write operations\n",
    "fig8 = visualizer.plot_memory_read_write_operations(\n",
    "    operation_results['initial_read'],\n",
    "    operation_results['initial_write'],\n",
    "    memory_slots=[f\"Slot{i}\" for i in range(ntm_memory.mem_size)],\n",
    "    title=\"Initial Memory Read/Write Attention Weights\"\n",
    ")\n",
    "\n",
    "print(\"Initial read/write operations visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final read/write operations\n",
    "fig9 = visualizer.plot_memory_read_write_operations(\n",
    "    operation_results['final_read'],\n",
    "    operation_results['final_write'],\n",
    "    memory_slots=[f\"Slot{i}\" for i in range(ntm_memory.mem_size)],\n",
    "    title=\"Final Memory Read/Write Attention Weights\"\n",
    ")\n",
    "\n",
    "print(\"Final read/write operations visualization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparative Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate attention patterns with different characteristics\n",
    "def generate_attention_patterns():\n",
    "    \"\"\"Generate various attention patterns for comparison.\"\"\"\n",
    "    size = 10\n",
    "    \n",
    "    # 1. Diagonal/Identity pattern (self-focused)\n",
    "    diagonal = torch.eye(size) + torch.randn(size, size) * 0.1\n",
    "    diagonal = torch.softmax(diagonal, dim=-1)\n",
    "    \n",
    "    # 2. Local pattern (neighbors)\n",
    "    local_pattern = torch.zeros(size, size)\n",
    "    for i in range(size):\n",
    "        for j in range(max(0, i-2), min(size, i+3)):\n",
    "            local_pattern[i, j] = 1.0\n",
    "    local_pattern = torch.softmax(local_pattern + torch.randn(size, size) * 0.1, dim=-1)\n",
    "    \n",
    "    # 3. Global pattern (uniform)\n",
    "    global_pattern = torch.ones(size, size) / size\n",
    "    \n",
    "    # 4. Random pattern\n",
    "    random_pattern = torch.softmax(torch.randn(size, size), dim=-1)\n",
    "    \n",
    "    return {\n",
    "        'diagonal': diagonal,\n",
    "        'local': local_pattern,\n",
    "        'global': global_pattern,\n",
    "        'random': random_pattern\n",
    "}\n",
    "\n",
    "# Generate patterns\n",
    "attention_patterns = generate_attention_patterns()\n",
    "print(\"Attention patterns generated for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all patterns side by side\n",
    "fig10 = visualizer.plot_attention_patterns(\n",
    "    torch.stack(list(attention_patterns.values())),\n",
    "    pattern_names=list(attention_patterns.keys()),\n",
    "    title=\"Comparative Attention Patterns\"\n",
    ")\n",
    "\n",
    "print(\"Comparative attention patterns visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze pattern characteristics\n",
    "def analyze_attention_pattern(pattern, name):\n",
    "    \"\"\"Analyze characteristics of an attention pattern.\"\"\"\n",
    "    pattern_np = pattern.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate entropy (measure of focus/spread)\n",
    "    entropy = -np.sum(pattern_np * np.log(pattern_np + 1e-8))\n",
    "    \n",
    "    # Calculate max attention\n",
    "    max_attention = np.max(pattern_np)\n",
    "    \n",
    "    # Calculate sparsity (fraction of small values)\n",
    "    sparsity = np.mean(pattern_np < 0.01)\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'entropy': entropy,\n",
    "        'max_attention': max_attention,\n",
    "        'sparsity': sparsity\n",
    "}\n",
    "\n",
    "# Analyze all patterns\n",
    "pattern_analysis = []\n",
    "for name, pattern in attention_patterns.items():\n",
    "    analysis = analyze_attention_pattern(pattern, name)\n",
    "    pattern_analysis.append(analysis)\n",
    "    \n",
    "# Display analysis\n",
    "import pandas as pd\n",
    "analysis_df = pd.DataFrame(pattern_analysis)\n",
    "analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Visualization Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization data\n",
    "comprehensive_data = {\n",
    "    'attention_weights': self_attention,\n",
    "    'token_state_attention': token_state_attention,\n",
    "    'state_attention': state_state_attention,\n",
    "    'memory_content': memory_content,\n",
    "    'importance_scores': importance_scores,\n",
    "    'initial_read_weights': operation_results['initial_read'],\n",
    "    'initial_write_weights': operation_results['initial_write'],\n",
    "    'multi_head_attention': multi_head_attention\n",
    "}\n",
    "\n",
    "# Generate comprehensive report\n",
    "report_dir = visualizer.create_comprehensive_report(\n",
    "    comprehensive_data,\n",
    "    save_dir=\"memory_attention_reports\"\n",
    ")\n",
    "\n",
    "print(f\"Comprehensive visualization report generated in: {report_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Memory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive analysis of memory content\n",
    "def analyze_memory_content(memory_tensor):\n",
    "    \"\"\"Provide detailed analysis of memory content.\"\"\"\n",
    "    memory_np = memory_tensor.detach().cpu().numpy()\n",
    "    \n",
    "    analysis = {\n",
    "        'shape': memory_np.shape,\n",
    "        'total_elements': memory_np.size,\n",
    "        'mean': np.mean(memory_np),\n",
    "        'std': np.std(memory_np),\n",
    "        'min': np.min(memory_np),\n",
    "        'max': np.max(memory_np),\n",
    "        'sparsity': np.mean(np.abs(memory_np) < 0.01),  # Fraction near zero\n",
    "        'active_slots': np.mean(np.std(memory_np, axis=1) > 0.1)  # Slots with variation\n",
    "}\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze memory\n",
    "memory_analysis = analyze_memory_content(memory_content)\n",
    "print(\"Memory Content Analysis:\")\n",
    "for key, value in memory_analysis.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive attention analysis\n",
    "def analyze_attention_content(attention_tensor, name=\"Attention\"):\n",
    "    \"\"\"Provide detailed analysis of attention content.\"\"\"\n",
    "    attention_np = attention_tensor.detach().cpu().numpy()\n",
    "    \n",
    "    # For 2D attention matrices\n",
    "    if attention_np.ndim == 2:\n",
    "        # Calculate row-wise entropy (focus per query)\n",
    "        row_entropy = -np.sum(attention_np * np.log(attention_np + 1e-8), axis=1)\n",
    "        \n",
    "        # Calculate column-wise entropy (focus per key)\n",
    "        col_entropy = -np.sum(attention_np * np.log(attention_np + 1e-8), axis=0)\n",
    "        \n",
    "        analysis = {\n",
    "            'name': name,\n",
    "            'shape': attention_np.shape,\n",
    "            'mean': np.mean(attention_np),\n",
    "            'std': np.std(attention_np),\n",
    "            'max': np.max(attention_np),\n",
    "            'min': np.min(attention_np),\n",
    "            'avg_row_entropy': np.mean(row_entropy),\n",
    "            'avg_col_entropy': np.mean(col_entropy),\n",
    "            'focus_score': 1.0 - np.mean(row_entropy) / np.log(attention_np.shape[1])  # Normalized focus\n",
    "}\n",
    "    else:\n",
    "        analysis = {\n",
    "            'name': name,\n",
    "            'shape': attention_np.shape,\n",
    "            'mean': np.mean(attention_np),\n",
    "            'std': np.std(attention_np),\n",
    "            'max': np.max(attention_np),\n",
    "            'min': np.min(attention_np)\n",
    "}\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze different attention types\n",
    "attention_analysis = []\n",
    "attention_analysis.append(analyze_attention_content(self_attention, \"Self-Attention\"))\n",
    "attention_analysis.append(analyze_attention_content(token_state_attention, \"Token-State Attention\"))\n",
    "attention_analysis.append(analyze_attention_content(state_state_attention, \"State-State Attention\"))\n",
    "\n",
    "# Display analysis\n",
    "attention_df = pd.DataFrame(attention_analysis)\n",
    "attention_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates comprehensive visualization tools for memory and attention in Neural State Machine models:\n",
    "\n",
    "### Attention Visualization:\n",
    "1. **Self-Attention Patterns** - Traditional attention between sequence elements\n",
    "2. **Token-to-State Routing** - How input tokens are distributed to state nodes\n",
    "3. **State-to-State Communication** - Interaction patterns between states\n",
    "4. **Multi-Head Attention** - Comparison of different attention heads\n",
    "5. **Comparative Patterns** - Analysis of different attention characteristics\n",
    "\n",
    "### Memory Visualization:\n",
    "1. **Memory Content Heatmaps** - Visualization of external memory values\n",
    "2. **Memory Evolution** - How memory changes over time\n",
    "3. **Memory Importance** - Slot-wise importance scoring\n",
    "4. **Read/Write Operations** - Visualization of memory access patterns\n",
    "5. **Interactive Analysis** - Detailed statistical analysis of memory and attention\n",
    "\n",
    "### Key Features:\n",
    "- **Multiple Visualization Types**: Heatmaps, line plots, bar charts\n",
    "- **Statistical Analysis**: Entropy, sparsity, focus metrics\n",
    "- **Comparative Analysis**: Side-by-side pattern comparison\n",
    "- **Automated Reporting**: Comprehensive report generation\n",
    "- **Interactive Exploration**: Detailed data analysis tools\n",
    "\n",
    "These visualization tools help interpret the internal mechanisms of NSM models and provide insights into their memory management and attention processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}