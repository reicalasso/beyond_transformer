{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Count Hyperparameter Sweep\n",
    "\n",
    "This notebook implements a hyperparameter sweep to test different numbers of state nodes (8, 16, 32, 64) across multiple datasets (MNIST, Tiny Shakespeare, IMDb). We'll record accuracy, memory usage, and training speed, then plot performance vs state count for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from nsm.state_propagator import StatePropagator\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Simple Model for Testing\n",
    "\n",
    "We'll create a simple model that uses our StatePropagator to test with different state counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNSMModel(nn.Module):\n",
    "    \"\"\"A simple model using StatePropagator for hyperparameter testing.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, state_dim, num_states, output_dim, gate_type='gru'):\n",
    "        super(SimpleNSMModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.num_states = num_states\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Input embedding to state dimension\n",
    "        self.input_embedding = nn.Embedding(input_dim, state_dim) if input_dim > 1000 else nn.Linear(input_dim, state_dim)\n",
    "        \n",
    "        # State propagator\n",
    "        self.state_propagator = StatePropagator(\n",
    "            state_dim=state_dim, \n",
    "            gate_type=gate_type,\n",
    "            enable_communication=True\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(state_dim * num_states, output_dim)\n",
    "        \n",
    "        # Initialize states\n",
    "        self.initial_states = nn.Parameter(torch.randn(1, num_states, state_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len] for discrete data or [batch_size, input_dim] for continuous\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Handle different input types\n",
    "        if x.dim() == 2 and x.size(1) > 1000:  # Discrete sequences (text)\n",
    "            seq_len = x.size(1)\n",
    "            # Embed input to state dimension\n",
    "            x = self.input_embedding(x)  # [batch_size, seq_len, state_dim]\n",
    "        elif x.dim() == 2:  # Continuous features (images)\n",
    "            # Project input to state dimension\n",
    "            x = self.input_embedding(x).unsqueeze(1)  # [batch_size, 1, state_dim]\n",
    "            seq_len = 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input shape: {x.shape}\")\n",
    "        \n",
    "        # Initialize states for batch\n",
    "        states = self.initial_states.repeat(batch_size, 1, 1)  # [batch_size, num_states, state_dim]\n",
    "        \n",
    "        # Process sequence\n",
    "        for t in range(seq_len):\n",
    "            # For simplicity, we'll use the same input for all states\n",
    "            input_t = x[:, t, :].unsqueeze(1).repeat(1, self.num_states, 1)  # [batch_size, num_states, state_dim]\n",
    "            \n",
    "            # Update states\n",
    "            states = self.state_propagator(states, input_t)\n",
    "        \n",
    "        # Global pooling of states\n",
    "        pooled_states = states.view(batch_size, -1)  # [batch_size, state_dim * num_states]\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_projection(pooled_states)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Datasets\n",
    "\n",
    "We'll create synthetic datasets that mimic the structure of MNIST, Tiny Shakespeare, and IMDb for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mnist_like_data(num_samples=1000):\n",
    "    \"\"\"Create MNIST-like synthetic data (28x28 grayscale images).\"\"\"\n",
    "    # MNIST: 28x28 = 784 pixels, 10 classes\n",
    "    X = torch.randn(num_samples, 784)\n",
    "    y = torch.randint(0, 10, (num_samples,))\n",
    "    return torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "def create_tiny_shakespeare_like_data(num_samples=1000, seq_len=256):\n",
    "    \"\"\"Create Tiny Shakespeare-like synthetic data (sequences of characters).\"\"\"\n",
    "    # Vocabulary size for characters (simplified)\n",
    "    vocab_size = 100\n",
    "    X = torch.randint(0, vocab_size, (num_samples, seq_len))\n",
    "    # For language modeling, target is next character\n",
    "    y = X[:, -1]  # Just predict last character for simplicity\n",
    "    return torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "def create_imdb_like_data(num_samples=1000, seq_len=512):\n",
    "    \"\"\"Create IMDb-like synthetic data (sequences of words).\"\"\"\n",
    "    # Vocabulary size for words (simplified)\n",
    "    vocab_size = 10000\n",
    "    X = torch.randint(0, vocab_size, (num_samples, seq_len))\n",
    "    # Binary sentiment classification\n",
    "    y = torch.randint(0, 2, (num_samples,))\n",
    "    return torch.utils.data.TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, epochs=3, lr=0.001):\n",
    "    \"\"\"Train the model and return metrics.\"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Memory tracking\n",
    "    process = psutil.Process(os.getpid())\n",
    "    initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Training time tracking\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            # Limit training for demonstration\n",
    "            if batch_idx > 10:  # Just for quick testing\n",
    "                break\n",
    "    \n",
    "    end_time = time.time()\n",
    "    final_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Calculate metrics\n",
    "    training_time = end_time - start_time\n",
    "    memory_usage = final_memory - initial_memory\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'memory_usage': memory_usage,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate the model and return accuracy.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            # Limit evaluation for demonstration\n",
    "            if total > 100:  # Just for quick testing\n",
    "                break\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Sweep Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparameter_sweep():\n",
    "    \"\"\"Run hyperparameter sweep for different state counts.\"\"\"\n",
    "    \n",
    "    # State counts to test\n",
    "    state_counts = [8, 16, 32, 64]\n",
    "    \n",
    "    # Results storage\n",
    "    results = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Dataset configurations\n",
    "    datasets = {\n",
    "        'MNIST': {\n",
    "            'data_func': create_mnist_like_data,\n",
    "            'input_dim': 784,\n",
    "            'output_dim': 10,\n",
    "            'seq_len': 1\n",
    "        },\n",
    "        'Tiny_Shakespeare': {\n",
    "            'data_func': create_tiny_shakespeare_like_data,\n",
    "            'input_dim': 100,  # vocab_size\n",
    "            'output_dim': 100,  # vocab_size\n",
    "            'seq_len': 256\n",
    "        },\n",
    "        'IMDb': {\n",
    "            'data_func': create_imdb_like_data,\n",
    "            'input_dim': 10000,  # vocab_size\n",
    "            'output_dim': 2,     # binary classification\n",
    "            'seq_len': 512\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Fixed parameters\n",
    "    state_dim = 128\n",
    "    epochs = 3\n",
    "    batch_size = 32\n",
    "    \n",
    "    for dataset_name, dataset_config in datasets.items():\n",
    "        print(f\"\\n=== Testing {dataset_name} ===\")\n",
    "        \n",
    "        # Create dataset\n",
    "        train_dataset = dataset_config['data_func'](num_samples=500)  # Smaller for demo\n",
    "        test_dataset = dataset_config['data_func'](num_samples=100)   # Smaller for demo\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for num_states in state_counts:\n",
    "            print(f\"  Testing with {num_states} states...\")\n",
    "            \n",
    "            # Create model\n",
    "            model = SimpleNSMModel(\n",
    "                input_dim=dataset_config['input_dim'],\n",
    "                state_dim=state_dim,\n",
    "                num_states=num_states,\n",
    "                output_dim=dataset_config['output_dim']\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            metrics = train_model(model, train_loader, epochs=epochs)\n",
    "            \n",
    "            # Evaluate model\n",
    "            test_accuracy = evaluate_model(model, test_loader)\n",
    "            \n",
    "            # Store results\n",
    "            results[dataset_name]['state_counts'].append(num_states)\n",
    "            results[dataset_name]['accuracies'].append(metrics['accuracy'])\n",
    "            results[dataset_name]['test_accuracies'].append(test_accuracy)\n",
    "            results[dataset_name]['memory_usages'].append(metrics['memory_usage'])\n",
    "            results[dataset_name]['training_times'].append(metrics['training_time'])\n",
    "            \n",
    "            print(f\"    Train Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "            print(f\"    Test Accuracy: {test_accuracy:.2f}%\")\n",
    "            print(f\"    Memory Usage: {metrics['memory_usage']:.2f} MB\")\n",
    "            print(f\"    Training Time: {metrics['training_time']:.2f} seconds\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Sweep and Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter sweep\n",
    "results = run_hyperparameter_sweep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    \"\"\"Plot the results of the hyperparameter sweep.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('State Count Hyperparameter Sweep Results', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Training Accuracy vs State Count\n",
    "    ax = axes[0, 0]\n",
    "    for dataset_name, data in results.items():\n",
    "        ax.plot(data['state_counts'], data['accuracies'], marker='o', label=dataset_name)\n",
    "    ax.set_xlabel('Number of State Nodes')\n",
    "    ax.set_ylabel('Training Accuracy (%)')\n",
    "    ax.set_title('Training Accuracy vs State Count')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Plot 2: Test Accuracy vs State Count\n",
    "    ax = axes[0, 1]\n",
    "    for dataset_name, data in results.items():\n",
    "        ax.plot(data['state_counts'], data['test_accuracies'], marker='o', label=dataset_name)\n",
    "    ax.set_xlabel('Number of State Nodes')\n",
    "    ax.set_ylabel('Test Accuracy (%)')\n",
    "    ax.set_title('Test Accuracy vs State Count')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Plot 3: Memory Usage vs State Count\n",
    "    ax = axes[1, 0]\n",
    "    for dataset_name, data in results.items():\n",
    "        ax.plot(data['state_counts'], data['memory_usages'], marker='o', label=dataset_name)\n",
    "    ax.set_xlabel('Number of State Nodes')\n",
    "    ax.set_ylabel('Memory Usage (MB)')\n",
    "    ax.set_title('Memory Usage vs State Count')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Plot 4: Training Time vs State Count\n",
    "    ax = axes[1, 1]\n",
    "    for dataset_name, data in results.items():\n",
    "        ax.plot(data['state_counts'], data['training_times'], marker='o', label=dataset_name)\n",
    "    ax.set_xlabel('Number of State Nodes')\n",
    "    ax.set_ylabel('Training Time (seconds)')\n",
    "    ax.set_title('Training Time vs State Count')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the results\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Finally, let's save our results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "serializable_results = {}\n",
    "for dataset_name, data in results.items():\n",
    "    serializable_results[dataset_name] = {\n",
    "        'state_counts': [int(x) for x in data['state_counts']],\n",
    "        'accuracies': [float(x) for x in data['accuracies']],\n",
    "        'test_accuracies': [float(x) for x in data['test_accuracies']],\n",
    "        'memory_usages': [float(x) for x in data['memory_usages']],\n",
    "        'training_times': [float(x) for x in data['training_times']]\n",
    "    }\n",
    "\n",
    "# Save results\n",
    "with open('hyperparameter_sweep_results.json', 'w') as f:\n",
    "    json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to hyperparameter_sweep_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a hyperparameter sweep to test different numbers of state nodes (8, 16, 32, 64) across three datasets (MNIST, Tiny Shakespeare, IMDb). We recorded:\n",
    "\n",
    "1. Training accuracy\n",
    "2. Test accuracy\n",
    "3. Memory usage\n",
    "4. Training time\n",
    "\n",
    "The results are plotted to visualize the performance vs state count for each dataset. This will help us understand the trade-offs between model complexity (number of state nodes) and performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}