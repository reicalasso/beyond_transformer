{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hybrid_attention_header",
   "metadata": {},
   "source": [
    "# Tutorial: Hybrid Attention in NSM\n",
    "\n",
    "This tutorial explains the **Hybrid Attention** mechanism in Neural State Machines (NSM), which combines local (token-token) and global (token-state) attention for comprehensive context modeling.\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will understand:\n",
    "\n",
    "1. The concept of hybrid attention in NSM\n",
    "2. How local and global attention complement each other\n",
    "3. The benefits of combining both attention types\n",
    "4. A simple implementation of hybrid attention\n",
    "\n",
    "## 🧠 What is Hybrid Attention?\n",
    "\n",
    "Traditional Transformers rely solely on token-token attention, which can miss global context. NSM introduces **hybrid attention** that combines:\n",
    "\n",
    "- **Local Attention**: Captures immediate, token-level context\n",
    "- **Global Attention**: Captures long-term, state-level context\n",
    "\n",
    "This dual approach enables the model to:\n",
    "- Understand fine-grained details (local)\n",
    "- Maintain big-picture understanding (global)\n",
    "- Balance efficiency with expressivity\n",
    "\n",
    "## 🔍 How Hybrid Attention Works\n",
    "\n",
    "The hybrid attention process involves:\n",
    "\n",
    "1. **Local Attention Computation**: Standard self-attention between tokens\n",
    "2. **Global Attention Computation**: Attention from tokens to state nodes\n",
    "3. **Context Combination**: Merging local and global contexts\n",
    "\n",
    "Let's implement a simple example to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For better visualization\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "batch_size = 1\n",
    "seq_length = 6  # Number of tokens\n",
    "token_dim = 16\n",
    "num_states = 4\n",
    "state_dim = 16\n",
    "\n",
    "# Create random token embeddings\n",
    "tokens = torch.randn(batch_size, seq_length, token_dim)\n",
    "\n",
    "# Create random state embeddings\n",
    "states = torch.randn(batch_size, num_states, state_dim)\n",
    "\n",
    "# Create routing weights (from previous tutorial)\n",
    "routing_weights = F.softmax(torch.bmm(tokens, states.transpose(1, 2)), dim=-1)\n",
    "\n",
    "print(f\"Token embeddings shape: {tokens.shape}\")\n",
    "print(f\"State embeddings shape: {states.shape}\")\n",
    "print(f\"Routing weights shape: {routing_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local_attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Attention (Token-Token)\n",
    "\n",
    "# Initialize a simple multi-head attention mechanism\n",
    "local_attn = nn.MultiheadAttention(token_dim, num_heads=2, batch_first=True)\n",
    "\n",
    "# Compute local attention\n",
    "local_context, local_weights = local_attn(tokens, tokens, tokens)\n",
    "\n",
    "print(f\"Local context shape: {local_context.shape}\")\n",
    "print(f\"Local attention weights shape: {local_weights.shape}\")\n",
    "\n",
    "# Visualize local attention for first token\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(local_weights[0].detach().numpy(), annot=True, cmap=\"YlGnBu\", fmt=\".2f\",\n",
    "            xticklabels=[f'Token {i}' for i in range(seq_length)],\n",
    "            yticklabels=[f'Token {i}' for i in range(seq_length)])\n",
    "plt.title('Local Attention (Token-Token)')\n",
    "plt.xlabel('Key Tokens')\n",
    "plt.ylabel('Query Tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global_attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Attention (Token-State)\n",
    "\n",
    "# Compute global context using routing weights\n",
    "# [B, N, S] x [B, S, D_state] -> [B, N, D_state]\n",
    "global_context = torch.bmm(routing_weights, states)\n",
    "\n",
    "print(f\"Global context shape: {global_context.shape}\")\n",
    "\n",
    "# Visualize global attention (routing weights)\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(routing_weights[0].detach().numpy(), annot=True, cmap=\"YlOrRd\", fmt=\".2f\",\n",
    "            xticklabels=[f'State {i}' for i in range(num_states)],\n",
    "            yticklabels=[f'Token {i}' for i in range(seq_length)])\n",
    "plt.title('Global Attention (Token-State)')\n",
    "plt.xlabel('State Nodes')\n",
    "plt.ylabel('Tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "context_combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Local and Global Context\n",
    "\n",
    "# Simple combination: weighted sum\n",
    "# In practice, this could be a more complex gating mechanism\n",
    "alpha = 0.5  # Balance parameter\n",
    "combined_context = alpha * local_context + (1 - alpha) * global_context\n",
    "\n",
    "print(f\"Combined context shape: {combined_context.shape}\")\n",
    "\n",
    "print(\"\\nComparison of contexts for first token:\")\n",
    "print(f\"Original token: {tokens[0, 0]}\")\n",
    "print(f\"Local context: {local_context[0, 0]}\")\n",
    "print(f\"Global context: {global_context[0, 0]}\")\n",
    "print(f\"Combined context: {combined_context[0, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_takeaways",
   "metadata": {},
   "source": [
    "## 🎓 Key Takeaways\n",
    "\n",
    "1. **Dual Context Modeling**: Hybrid attention captures both immediate (local) and long-term (global) context.\n",
    "2. **Complementary Information**: Local attention handles fine-grained details, while global attention maintains big-picture understanding.\n",
    "3. **Flexibility**: The combination mechanism can be learned or adjusted for different tasks.\n",
    "\n",
    "## 🚀 Next Steps\n",
    "\n",
    "With these foundational concepts understood, you're ready to explore the full NSM architecture.\n",
    "\n",
    "See `notebooks/research/nsm_prototype.ipynb` for a complete implementation of an NSM layer.\n",
    "\n",
    "For a deeper dive into research aspects, check out:\n",
    "- `notebooks/research/benchmarking.ipynb`: Performance comparisons\n",
    "- `notebooks/research/interpretability.ipynb`: Visualization of state evolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}