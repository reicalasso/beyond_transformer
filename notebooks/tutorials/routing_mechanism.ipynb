{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "routing_header",
   "metadata": {},
   "source": [
    "# Tutorial: Token-to-State Routing in NSM\n",
    "\n",
    "This tutorial explains the **Token-to-State Routing** mechanism in Neural State Machines (NSM), which determines how input tokens interact with state nodes.\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will understand:\n",
    "\n",
    "1. The concept of routing in NSM\n",
    "2. How tokens determine which states to interact with\n",
    "3. The efficiency benefits of selective routing\n",
    "4. A simple implementation of routing mechanism\n",
    "\n",
    "## 🧠 What is Token-to-State Routing?\n",
    "\n",
    "In traditional Transformers, each token attends to all other tokens, leading to O(n²) complexity. In NSM, tokens use a **routing mechanism** to selectively attend to only the most relevant state nodes.\n",
    "\n",
    "This approach:\n",
    "- Reduces computational complexity\n",
    "- Enables focused processing\n",
    "- Mimics goal-directed attention in biological systems\n",
    "\n",
    "## 🔍 How Routing Works\n",
    "\n",
    "The routing process involves:\n",
    "\n",
    "1. **Compatibility Scoring**: Compute similarity between tokens and states\n",
    "2. **Attention Weights**: Determine which states are most relevant\n",
    "3. **Selective Interaction**: Tokens interact only with top-k states\n",
    "\n",
    "Let's implement a simple example to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For better visualization\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "batch_size = 1\n",
    "seq_length = 8  # Number of tokens\n",
    "token_dim = 16\n",
    "num_states = 5\n",
    "state_dim = 16\n",
    "\n",
    "# Create random token embeddings\n",
    "tokens = torch.randn(batch_size, seq_length, token_dim)\n",
    "\n",
    "# Create random state embeddings\n",
    "states = torch.randn(batch_size, num_states, state_dim)\n",
    "\n",
    "print(f\"Token embeddings shape: {tokens.shape}\")\n",
    "print(f\"State embeddings shape: {states.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "routing_mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple routing mechanism (attention-based)\n",
    "\n",
    "# Compute compatibility scores (dot product)\n",
    "# [B, N, D] x [B, D, S] -> [B, N, S]\n",
    "compatibility = torch.bmm(tokens, states.transpose(1, 2))\n",
    "\n",
    "# Apply softmax to get attention weights\n",
    "routing_weights = F.softmax(compatibility, dim=-1)\n",
    "\n",
    "print(f\"Compatibility scores shape: {compatibility.shape}\")\n",
    "print(f\"Routing weights shape: {routing_weights.shape}\")\n",
    "print(\"\\nRouting weights for first token:\")\n",
    "print(routing_weights[0, 0, :])\n",
    "print(f\"\\nSum of weights for first token: {routing_weights[0, 0, :].sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "routing_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize routing weights\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "weights_np = routing_weights[0].detach().numpy()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(weights_np, annot=True, cmap=\"YlGnBu\", fmt=\".2f\",\n",
    "            xticklabels=[f'State {i}' for i in range(num_states)],\n",
    "            yticklabels=[f'Token {i}' for i in range(seq_length)])\n",
    "plt.title('Token-to-State Routing Weights')\n",
    "plt.xlabel('State Nodes')\n",
    "plt.ylabel('Tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "topk_routing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of top-k routing (for efficiency)\n",
    "\n",
    "# Get top-2 states for each token\n",
    "topk_values, topk_indices = torch.topk(routing_weights, k=2, dim=-1)\n",
    "\n",
    "print(\"Top-2 states for each token:\")\n",
    "for i in range(seq_length):\n",
    "    print(f\"Token {i}: States {topk_indices[0, i].tolist()} with weights {topk_values[0, i].tolist()}\")\n",
    "\n",
    "# This shows how each token can selectively interact with only the most relevant states\n",
    "# instead of all states, improving efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_takeaways",
   "metadata": {},
   "source": [
    "## 🎓 Key Takeaways\n",
    "\n",
    "1. **Selective Attention**: Tokens attend only to relevant states, not all states.\n",
    "2. **Efficiency**: This selective routing reduces computational complexity from O(n×s) to O(n×k) where k≪s.\n",
    "3. **Biological Plausibility**: Mimics how biological systems focus attention on relevant memory fragments.\n",
    "\n",
    "## 🚀 Next Tutorial\n",
    "\n",
    "In the next tutorial, we'll explore **Hybrid Attention**, which combines local and global attention mechanisms.\n",
    "\n",
    "See `notebooks/tutorials/hybrid_attention.ipynb` for the next part."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}