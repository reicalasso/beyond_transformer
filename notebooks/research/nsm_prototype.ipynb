{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pulse_prototype_header",
   "metadata": {},
   "source": [
    "# Parallel Unified Linear State Engine (pulse) Prototype\n",
    "\n",
    "This notebook implements a basic prototype of the Parallel Unified Linear State Engine (pulse) layer, demonstrating the core components and their interactions.\n",
    "\n",
    "## Objective\n",
    "\n",
    "To implement and test a minimal pulse layer that includes:\n",
    "\n",
    "1. **State Management**\n",
    "2. **Token-to-State Routing**\n",
    "3. **State Propagation**\n",
    "4. **Hybrid Attention**\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Input Tokens → Local Attention → Token-to-State Interaction → Updated States  \n",
    "↘ State-to-State Propagation ↙\n",
    "```\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- **Framework**: PyTorch\n",
    "- **State Nodes**: Fixed number of learnable memory slots\n",
    "- **Routing**: Soft attention mechanism\n",
    "- **Propagation**: Simple recurrent update\n",
    "- **Attention**: Combination of local and global mechanisms\n",
    "\n",
    "Let's start by implementing the core components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For better visualization\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "state_manager",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateManager(nn.Module):\n",
    "    \"\"\"\n",
    "    State Manager for Parallel Unified Linear State Engines (pulse).\n",
    "    Manages a fixed number of state vectors that evolve over time.\n",
    "    Each state vector has a dimension of D.\n",
    "    For a batch of size B, it manages [B, S, D] tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states, state_dim):\n",
    "        super(StateManager, self).__init__()\n",
    "        self.num_states = num_states\n",
    "        self.state_dim = state_dim\n",
    "\n",
    "    def forward(self, initial_states=None, batch_size=None):\n",
    "        \"\"\"\n",
    "        Returns initial states.\n",
    "        Args:\n",
    "            initial_states (torch.Tensor, optional): Predefined initial states of shape [B, S, D].\n",
    "            batch_size (int, optional): Batch size to create initial states if not provided.\n",
    "        Returns:\n",
    "            torch.Tensor: Initial states of shape [B, S, D].\n",
    "        \"\"\"\n",
    "        if initial_states is not None:\n",
    "            return initial_states\n",
    "        elif batch_size is not None:\n",
    "            # Initialize with zeros\n",
    "            return torch.zeros(batch_size, self.num_states, self.state_dim)\n",
    "        else:\n",
    "            raise ValueError(\"Either initial_states or batch_size must be provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "token_to_state_router",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenToStateRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Routes token embeddings to relevant state nodes.\n",
    "    Computes attention weights between tokens and states to determine routing.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_dim, state_dim, use_softmax=True):\n",
    "        super(TokenToStateRouter, self).__init__()\n",
    "        self.token_dim = token_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.use_softmax = use_softmax\n",
    "        # A linear layer to project token embeddings to state dimension for compatibility\n",
    "        self.token_to_state_proj = nn.Linear(token_dim, state_dim)\n",
    "\n",
    "    def forward(self, token_embeddings, state_embeddings):\n",
    "        \"\"\"\n",
    "        Compute routing weights from tokens to states.\n",
    "        Args:\n",
    "            token_embeddings (torch.Tensor): [B, N, D_token]\n",
    "            state_embeddings (torch.Tensor): [B, S, D_state]\n",
    "        Returns:\n",
    "            torch.Tensor: Routing weights of shape [B, N, S]\n",
    "        \"\"\"\n",
    "        # Project tokens to state dimension\n",
    "        projected_tokens = self.token_to_state_proj(token_embeddings)  # [B, N, D_state]\n",
    "        # Compute compatibility scores (dot product)\n",
    "        # [B, N, D_state] x [B, D_state, S] -> [B, N, S]\n",
    "        compatibility = torch.bmm(projected_tokens, state_embeddings.transpose(1, 2))\n",
    "        if self.use_softmax:\n",
    "            routing_weights = F.softmax(compatibility, dim=-1)\n",
    "        else:\n",
    "            routing_weights = compatibility\n",
    "        return routing_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "state_propagator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatePropagator(nn.Module):\n",
    "    \"\"\"\n",
    "    Updates state embeddings across layers.\n",
    "    Implements a simple recurrent update mechanism for state nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim=None):\n",
    "        super(StatePropagator, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.hidden_dim = hidden_dim or state_dim\n",
    "        # For simplicity, we'll use a linear transformation for update\n",
    "        self.update_layer = nn.Linear(state_dim, state_dim)\n",
    "\n",
    "    def forward(self, state_embeddings):\n",
    "        \"\"\"\n",
    "        Update state embeddings for the next layer.\n",
    "        Args:\n",
    "            state_embeddings (torch.Tensor): [B, S, D_state]\n",
    "        Returns:\n",
    "            torch.Tensor: Updated state embeddings of shape [B, S, D_state]\n",
    "        \"\"\"\n",
    "        # Simple update: add a transformed version of the current states\n",
    "        update = self.update_layer(state_embeddings)\n",
    "        new_states = state_embeddings + update\n",
    "        return new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hybrid_attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines local (token-token) and global (token-state) attention mechanisms.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_dim):\n",
    "        super(HybridAttention, self).__init__()\n",
    "        self.token_dim = token_dim\n",
    "        # Local attention mechanism (standard self-attention)\n",
    "        self.local_attn = nn.MultiheadAttention(token_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "    def forward(self, token_embeddings, state_embeddings, routing_weights):\n",
    "        \"\"\"\n",
    "        Apply hybrid attention.\n",
    "        Args:\n",
    "            token_embeddings (torch.Tensor): [B, N, D_token]\n",
    "            state_embeddings (torch.Tensor): [B, S, D_state]\n",
    "            routing_weights (torch.Tensor): [B, N, S] from TokenToStateRouter\n",
    "        Returns:\n",
    "            torch.Tensor: Context-enriched token embeddings of shape [B, N, D_token]\n",
    "        \"\"\"\n",
    "        # 1. Local Attention (token-token)\n",
    "        local_context, _ = self.local_attn(token_embeddings, token_embeddings, token_embeddings)\n",
    "        # 2. Global Attention (token-state)\n",
    "        # Use routing weights to aggregate state information\n",
    "        # [B, N, S] x [B, S, D_state] -> [B, N, D_state]\n",
    "        global_context = torch.bmm(routing_weights, state_embeddings)\n",
    "        # 3. Combine local and global context\n",
    "        # For simplicity, we'll sum them (assuming same dimensions)\n",
    "        if local_context.size(-1) == global_context.size(-1):\n",
    "            output = local_context + global_context\n",
    "        else:\n",
    "            # If dimensions differ, project to token_dim\n",
    "            if not hasattr(self, 'context_proj'):\n",
    "                self.context_proj = nn.Linear(local_context.size(-1) + global_context.size(-1), self.token_dim)\n",
    "            combined_context = torch.cat([local_context, global_context], dim=-1)\n",
    "            output = self.context_proj(combined_context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pulse_layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pulseLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of the Parallel Unified Linear State Engine (pulse).\n",
    "    Integrates all components: State Management, Routing, Propagation, and Hybrid Attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states, state_dim, token_dim):\n",
    "        super(pulseLayer, self).__init__()\n",
    "        self.num_states = num_states\n",
    "        self.state_dim = state_dim\n",
    "        self.token_dim = token_dim\n",
    "        # Initialize components\n",
    "        self.state_manager = StateManager(num_states, state_dim)\n",
    "        self.router = TokenToStateRouter(token_dim, state_dim)\n",
    "        self.propagator = StatePropagator(state_dim)\n",
    "        self.hybrid_attn = HybridAttention(token_dim)\n",
    "        # A layer norm for stability\n",
    "        self.layer_norm = nn.LayerNorm(token_dim)\n",
    "\n",
    "    def forward(self, token_embeddings, state_embeddings=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the pulse layer.\n",
    "        Args:\n",
    "            token_embeddings (torch.Tensor): [B, N, D_token]\n",
    "            state_embeddings (torch.Tensor, optional): [B, S, D_state]. If None, initialized.\n",
    "        Returns:\n",
    "            tuple: (updated_token_embeddings [B, N, D_token], updated_state_embeddings [B, S, D_state])\n",
    "        \"\"\"\n",
    "        batch_size = token_embeddings.size(0)\n",
    "        # 1. Initialize or use provided state embeddings\n",
    "        if state_embeddings is None:\n",
    "            state_embeddings = self.state_manager(batch_size=batch_size)\n",
    "        # 2. Token-to-State Routing\n",
    "        routing_weights = self.router(token_embeddings, state_embeddings)\n",
    "        # 3. Hybrid Attention\n",
    "        attended_tokens = self.hybrid_attn(token_embeddings, state_embeddings, routing_weights)\n",
    "        # 4. Residual connection and layer normalization for tokens\n",
    "        updated_tokens = self.layer_norm(token_embeddings + attended_tokens)\n",
    "        # 5. State Propagation\n",
    "        updated_states = self.propagator(state_embeddings)\n",
    "        return updated_tokens, updated_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "test_pulse_layer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens shape: torch.Size([2, 10, 32])\n",
      "Updated tokens shape: torch.Size([2, 10, 32])\n",
      "Updated states shape: torch.Size([2, 5, 32])\n",
      "\n",
      "✅ pulse Layer test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test the pulse Layer\n",
    "\n",
    "# Parameters\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "token_dim = 32\n",
    "num_states = 5\n",
    "state_dim = 32\n",
    "\n",
    "# Create random token embeddings\n",
    "tokens = torch.randn(batch_size, seq_length, token_dim)\n",
    "\n",
    "# Initialize pulse Layer\n",
    "pulse_layer = pulseLayer(num_states, state_dim, token_dim)\n",
    "\n",
    "# Forward pass\n",
    "updated_tokens, updated_states = pulse_layer(tokens)\n",
    "\n",
    "print(f\"Input tokens shape: {tokens.shape}\")\n",
    "print(f\"Updated tokens shape: {updated_tokens.shape}\")\n",
    "print(f\"Updated states shape: {updated_states.shape}\")\n",
    "\n",
    "# Check if shapes are as expected\n",
    "assert updated_tokens.shape == tokens.shape, \"Token shapes do not match\"\n",
    "assert updated_states.shape == (batch_size, num_states, state_dim), \"State shapes do not match\"\n",
    "\n",
    "print(\"\\n✅ pulse Layer test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This prototype demonstrates the core components of an pulse layer. In the next steps, we will:\n",
    "\n",
    "1. **Stack Multiple pulse Layers** to create a deeper architecture\n",
    "2. **Integrate with a Classification Head** for downstream tasks\n",
    "3. **Train on Simple Datasets** like MNIST to validate performance\n",
    "4. **Compare with Baseline Models** (Transformer, LSTM, etc.)\n",
    "\n",
    "See `notebooks/research/benchmarking.ipynb` for the next phase of research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
