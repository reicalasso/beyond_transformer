{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nsm_prototype_header",
   "metadata": {},
   "source": [
    "# Neural State Machine (NSM) Prototype\n",
    "\n",
    "This notebook implements a basic prototype of the Neural State Machine (NSM) layer, demonstrating the core components and their interactions.\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "\n",
    "To implement and test a minimal NSM layer that includes:\n",
    "\n",
    "1. **State Management**\n",
    "2. **Token-to-State Routing**\n",
    "3. **State Propagation**\n",
    "4. **Hybrid Attention**\n",
    "\n",
    "## ðŸ§  Architecture Overview\n",
    "\n",
    "```\n",
    "Input Tokens â†’ Local Attention â†’ Token-to-State Interaction â†’ Updated States  \n",
    "â†˜ State-to-State Propagation â†™\n",
    "```\n",
    "\n",
    "## ðŸ”§ Implementation Details\n",
    "\n",
    "- **Framework**: PyTorch\n",
    "- **State Nodes**: Fixed number of learnable memory slots\n",
    "- **Routing**: Soft attention mechanism\n",
    "- **Propagation**: Simple recurrent update\n",
    "- **Attention**: Combination of local and global mechanisms\n",
    "\n",
    "Let's start by implementing the core components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For better visualization\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state_manager",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateManager(nn.Module):\n",
    "    \"\"\"\n",
    "    State Manager for Neural State Machines (NSM).\n",
    "    \n",
    "    Manages a fixed number of state vectors that evolve over time.\n",
    "    Each state vector has a dimension of D.\n",
    "    For a batch of size B, it manages [B, S, D] tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states, state_dim):\n",
    "        super(StateManager, self).__init__()\n",
    "        self.num_states = num_states\n",
    "        self.state_dim = state_dim\n",
    "        # Initialize states as learnable parameters\n",
    "        # In a full model, these might be initialized per batch or with a more complex scheme\n",
    "        # But for a layer, the initial states could be learned or passed as input\n",
    "        \n",
    "    def forward(self, initial_states=None, batch_size=None):\n",
    "        \"\"\"\n",
    "        Returns initial states.\n",
    "        \n",
    "        Args:\n",
    "            initial_states (torch.Tensor, optional): Predefined initial states of shape [B, S, D].\n",
    "            batch_size (int, optional): Batch size to create initial states if not provided.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Initial states of shape [B, S, D].\n",
    "        \"\"\"\n",
    "        if initial_states is not None:\n",
    "            return initial_states\n",
    "        elif batch_size is not None:\n",
    "            # Initialize with zeros or small random values\n",
    "            return torch.zeros(batch_size, self.num_states, self.state_dim)\n",
    "        else:\n",
    "            raise ValueError(\"Either initial_states or batch_size must be provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "token_to_state_router",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenToStateRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Routes token embeddings to relevant state nodes.\n",
    "    \n",
    "    Computes attention weights between tokens and states to determine routing.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_dim, state_dim, use_softmax=True):\n",
    "        super(TokenToStateRouter, self).__init__()\n",
    "        self.token_dim = token_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.use_softmax = use_softmax\n",
    "        \n",
    "        # A linear layer to project token embeddings to state dimension for compatibility\n",
    "        self.token_to_state_proj = nn.Linear(token_dim, state_dim)\n",
    "        \n",
    "    def forward(self, token_embeddings, state_embeddings):\n",
    "        \"\"\"\n",
    "        Compute routing weights from tokens to states.\n",
    "        \n",
    "        Args:\n",
    "            token_embeddings (torch.Tensor): [B, N, D_token]\n",
    "            state_embeddings (torch.Tensor): [B, S, D_state]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Routing weights of shape [B, N, S]\n",
    "        \"\"\"\n",
    "        # Project tokens to state dimension\n",
    "        projected_tokens = self.token_to_state_proj(token_embeddings)  // [B, N, D_state]\n",
    "        \n",
    "        // Compute compatibility scores (dot product)\n",
    "        // [B, N, D_state] x [B, D_state, S] -> [B, N, S]\n",
    "        compatibility = torch.bmm(projected_tokens, state_embeddings.transpose(1, 2))\n",
    "        \n",
    "        if self.use_softmax:\n",
    "            // Apply softmax to get attention weights\n",
    "            routing_weights = F.softmax(compatibility, dim=-1)\n",
    "        else:\n",
    "            // Use raw compatibility scores (can be normalized differently)\n",
    "            routing_weights = compatibility\n",
    "            \n",
    "        return routing_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state_propagator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatePropagator(nn.Module):\n",
    "    \"\"\"\n",
    "    Updates state embeddings across layers.\n",
    "    \n",
    "    Implements a simple recurrent update mechanism for state nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim=None):\n",
    "        super(StatePropagator, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.hidden_dim = hidden_dim or state_dim\n",
    "        \n",
    "        // For simplicity, we'll use a linear transformation for update\n",
    "        // In a more complex version, this could be a GRUCell or other recurrent unit\n",
    "        self.update_layer = nn.Linear(state_dim, state_dim)\n",
    "        \n",
    "    def forward(self, state_embeddings):\n",
    "        \"\"\"\n",
    "        Update state embeddings for the next layer.\n",
    "        \n",
    "        Args:\n",
    "            state_embeddings (torch.Tensor): [B, S, D_state]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Updated state embeddings of shape [B, S, D_state]\n",
    "        \"\"\"\n",
    "        // Simple update: add a transformed version of the current states\n",
    "        // This is a placeholder for more complex dynamics\n",
    "        update = self.update_layer(state_embeddings)\n",
    "        new_states = state_embeddings + update\n",
    "        return new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid_attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines local (token-token) and global (token-state) attention mechanisms.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_dim):\n",
    "        super(HybridAttention, self).__init__()\n",
    "        self.token_dim = token_dim\n",
    "        // Local attention mechanism (standard self-attention)\n",
    "        self.local_attn = nn.MultiheadAttention(token_dim, num_heads=4, batch_first=True)\n",
    "        // For global attention, we'll assume token-to-state routing weights are used\n",
    "        \n",
    "    def forward(self, token_embeddings, state_embeddings, routing_weights):\n",
    "        \"\"\"\n",
    "        Apply hybrid attention.\n",
    "        \n",
    "        Args:\n",
    "            token_embeddings (torch.Tensor): [B, N, D_token]\n",
    "            state_embeddings (torch.Tensor): [B, S, D_state]\n",
    "            routing_weights (torch.Tensor): [B, N, S] from TokenToStateRouter\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Context-enriched token embeddings of shape [B, N, D_token]\n",
    "        \"\"\"\n",
    "        // 1. Local Attention (token-token)\n",
    "        local_context, _ = self.local_attn(token_embeddings, token_embeddings, token_embeddings)\n",
    "        \n",
    "        // 2. Global Attention (token-state)\n",
    "        // Use routing weights to aggregate state information\n",
    "        // [B, N, S] x [B, S, D_state] -> [B, N, D_state]\n",
    "        global_context = torch.bmm(routing_weights, state_embeddings)\n",
    "        \n",
    "        // 3. Combine local and global context\n",
    "        // For simplicity, we'll concatenate and project\n",
    "        // In a more advanced version, this could be a gated combination\n",
    "        combined_context = torch.cat([local_context, global_context], dim=-1)\n",
    "        // We need to project back to token_dim if dimensions don't match\n",
    "        // Assuming state_dim == token_dim for simplicity here\n",
    "        // Otherwise, add a projection layer\n",
    "        \n",
    "        // For now, just sum them (assuming same dimensions)\n",
    "        // In practice, you'd want to handle dimension mismatch properly\n",
    "        if local_context.size(-1) == global_context.size(-1):\n",
    "            output = local_context + global_context\n",
    "        else:\n",
    "            // Need to project, let's add a linear layer for this case\n",
    "            if not hasattr(self, 'context_proj'):\n",
    "                self.context_proj = nn.Linear(local_context.size(-1) + global_context.size(-1), self.token_dim)\n",
    "            output = self.context_proj(combined_context)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nsm_layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSMLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of the Neural State Machine (NSM).\n",
    "    \n",
    "    Integrates all components: State Management, Routing, Propagation, and Hybrid Attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states, state_dim, token_dim):\n",
    "        super(NSMLayer, self).__init__()\n",
    "        self.num_states = num_states\n",
    "        self.state_dim = state_dim\n",
    "        self.token_dim = token_dim\n",
    "        \n",
    "        // Initialize components\n",
    "        self.state_manager = StateManager(num_states, state_dim)\n",
    "        self.router = TokenToStateRouter(token_dim, state_dim)\n",
    "        self.propagator = StatePropagator(state_dim)\n",
    "        self.hybrid_attn = HybridAttention(token_dim)\n",
    "        \n",
    "        // A layer norm for stability\n",
    "        self.layer_norm = nn.LayerNorm(token_dim)\n",
    "        \n",
    "    def forward(self, token_embeddings, state_embeddings=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the NSM layer.\n",
    "        \n",
    "        Args:\n",
    "            token_embeddings (torch.Tensor): [B, N, D_token]\n",
    "            state_embeddings (torch.Tensor, optional): [B, S, D_state]. If None, initialized.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (updated_token_embeddings [B, N, D_token], updated_state_embeddings [B, S, D_state])\n",
    "        \"\"\"\n",
    "        batch_size = token_embeddings.size(0)\n",
    "        \n",
    "        // 1. Initialize or use provided state embeddings\n",
    "        if state_embeddings is None:\n",
    "            state_embeddings = self.state_manager(batch_size=batch_size)\n",
    "            \n",
    "        // 2. Token-to-State Routing\n",
    "        routing_weights = self.router(token_embeddings, state_embeddings)\n",
    "        \n",
    "        // 3. Hybrid Attention\n",
    "        attended_tokens = self.hybrid_attn(token_embeddings, state_embeddings, routing_weights)\n",
    "        \n",
    "        // 4. Residual connection and layer normalization for tokens\n",
    "        updated_tokens = self.layer_norm(token_embeddings + attended_tokens)\n",
    "        \n",
    "        // 5. State Propagation\n",
    "        updated_states = self.propagator(state_embeddings)\n",
    "        \n",
    "        return updated_tokens, updated_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_nsm_layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Test the NSM Layer\n",
    "\n",
    "// Parameters\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "token_dim = 32\n",
    "num_states = 5\n",
    "state_dim = 32\n",
    "\n",
    "// Create random token embeddings\n",
    "tokens = torch.randn(batch_size, seq_length, token_dim)\n",
    "\n",
    "// Initialize NSM Layer\n",
    "nsm_layer = NSMLayer(num_states, state_dim, token_dim)\n",
    "\n",
    "// Forward pass\n",
    "updated_tokens, updated_states = nsm_layer(tokens)\n",
    "\n",
    "print(f\"Input tokens shape: {tokens.shape}\")\n",
    "print(f\"Updated tokens shape: {updated_tokens.shape}\")\n",
    "print(f\"Updated states shape: {updated_states.shape}\")\n",
    "\n",
    "// Check if shapes are as expected\n",
    "assert updated_tokens.shape == tokens.shape, \"Token shapes do not match\"\n",
    "assert updated_states.shape == (batch_size, num_states, state_dim), \"State shapes do not match\"\n",
    "\n",
    "print(\"\\nâœ… NSM Layer test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "## ðŸš€ Next Steps\n",
    "\n",
    "This prototype demonstrates the core components of an NSM layer. In the next steps, we will:\n",
    "\n",
    "1. **Stack Multiple NSM Layers** to create a deeper architecture\n",
    "2. **Integrate with a Classification Head** for downstream tasks\n",
    "3. **Train on Simple Datasets** like MNIST to validate performance\n",
    "4. **Compare with Baseline Models** (Transformer, LSTM, etc.)\n",
    "\n",
    "See `notebooks/research/benchmarking.ipynb` for the next phase of research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}