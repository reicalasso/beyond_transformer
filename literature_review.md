# Literature Review

This document summarizes key research and limitations of Transformer architectures, including:
- Quadratic attention complexity for long sequences
- Sequence-only data representation limitations
- Fixed, non-adaptive attention mechanisms

Alternative approaches and modifications explored in literature are also documented.
