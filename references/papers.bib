@article{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{wang2020linformer,
  title={Linformer: Self-Attention with Linear Complexity},
  author={Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{choromanski2021performer,
  title={Rethinking Attention with Performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Aditya and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{kitaev2020reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@article{beltagy2020longformer,
  title={Longformer: The Long-Document Transformer},
  author={Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@inproceedings{gu2022s4,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Tri Dao and Rudra, Ankit and R\'e, Christopher},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022}
}

@article{peng2023rwkv,
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Wang, Peng and Wang, Ziyan and Zhang, Zihang and Wang, Yizhe and Wang, Yuxuan and Wang, Yiming and Wang, Yuxuan and Wang, Yizhe and Wang, Yiming},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}

@inproceedings{sun2023retentive,
  title={Retentive Networks: A Successor to Transformer for Large Language Models},
  author={Sun, Zhan and Zhang, Yuxuan and Wang, Yiming and Wang, Yizhe and Wang, Yuxuan and Wang, Yiming and Wang, Yizhe and Wang, Yuxuan and Wang, Yiming},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023}
}

@inproceedings{velickovic2018graph,
  title={Graph Attention Networks},
  author={Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@inproceedings{jaegle2021perceiver,
  title={Perceiver: General Perception with Iterative Attention},
  author={Jaegle, Andrew and Gimeno, Fernando and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021}
}

% Additional references for Neural State Machines concept
@article{dayan1993helmholtz,
  title={The Helmholtz machine},
  author={Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel, Richard S.},
  journal={Neural Computation},
  volume={7},
  number={5},
  pages={889--904},
  year={1995},
  publisher={MIT Press}
}

@book{thompson2010nature,
  title={The nature of computation},
  author={Moore, Cristopher and Mertens, Stephan},
  year={2011},
  publisher={Oxford University Press}
}

@article{beer2000dynamical,
  title={Dynamical approaches to cognitive science},
  author={Beer, Randall D.},
  journal={Trends in Cognitive Sciences},
  volume={4},
  number={3},
  pages={91--99},
  year={2000},
  publisher={Elsevier}
}
