"""
Simple test for new NSM components.
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
from nsm.modules.ssm_block import SSMBlock
from nsm.modules.ntm_memory import NTMMemory
from nsm.modules.transformer_attention import TransformerAttention
from nsm.modules.rnn_memory import RNNMemory


def test_components():
    """Test all new components."""
    print("Testing NSM Core Components...")
    
    # Test SSMBlock
    print("\n1. Testing SSMBlock...")
    ssm = SSMBlock(d_model=64, d_state=16, d_conv=4, expand=2)
    x = torch.randn(2, 10, 64)
    out = ssm(x)
    print(f"   Input: {x.shape} -> Output: {out.shape}")
    assert out.shape == x.shape
    print("   âœ“ SSMBlock OK")
    
    # Test NTMMemory
    print("\n2. Testing NTMMemory...")
    ntm = NTMMemory(mem_size=128, mem_dim=20, num_read_heads=1, num_write_heads=1)
    read_keys = torch.randn(2, 1, 20)
    write_keys = torch.randn(2, 1, 20)
    read_strengths = torch.randn(2, 1)
    write_strengths = torch.randn(2, 1)
    erase_vectors = torch.randn(2, 1, 20)
    add_vectors = torch.randn(2, 1, 20)
    
    read_vecs, mem_state = ntm(read_keys, write_keys, read_strengths, write_strengths, 
                              erase_vectors, add_vectors)
    print(f"   Read vectors: {read_vecs.shape}")
    print(f"   Memory state: {mem_state.shape}")
    assert read_vecs.shape == (2, 1, 20)
    assert mem_state.shape == (128, 20)
    print("   âœ“ NTMMemory OK")
    
    # Test TransformerAttention
    print("\n3. Testing TransformerAttention...")
    attn = TransformerAttention(d_model=64, num_heads=8)
    x = torch.randn(2, 10, 64)
    out, weights = attn.forward_self_attention(x)
    print(f"   Input: {x.shape} -> Output: {out.shape}")
    print(f"   Attention weights: {weights.shape}")
    assert out.shape == x.shape
    assert weights.shape == (2, 8, 10, 10)
    print("   âœ“ TransformerAttention OK")
    
    # Test RNNMemory
    print("\n4. Testing RNNMemory...")
    rnn = RNNMemory(input_dim=64, hidden_dim=128, num_layers=2, rnn_type='gru')
    x = torch.randn(2, 10, 64)
    hidden = rnn.init_hidden(2, torch.device('cpu'))
    out, hidden_out = rnn(x, hidden)
    print(f"   Input: {x.shape} -> Output: {out.shape}")
    print(f"   Hidden state: {type(hidden_out)}")
    assert out.shape == (2, 10, 128)
    print("   âœ“ RNNMemory OK")
    
    print("\nðŸŽ‰ All components tested successfully!")


if __name__ == "__main__":
    test_components()