#!/usr/bin/env python3
"""
Train PULSE on TinyStories dataset from Hugging Face.

TinyStories contains ~2.1M synthetic short stories generated by GPT-3.5/4.
This is ideal for training small language models that can generate coherent text.
"""

import argparse
import gc
import json
import logging
import math
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.amp import GradScaler, autocast
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

from datasets import load_dataset
from transformers import AutoTokenizer

from pulse.models.optimized_pulse_lm import OptimizedPulseConfig, OptimizedPulseForCausalLM

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)


class TinyStoriesDataset(Dataset):
    """Dataset wrapper for TinyStories."""
    
    def __init__(self, data, tokenizer, max_length: int = 256):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.examples = []
        
        logger.info("Tokenizing dataset...")
        for item in tqdm(data, desc="Processing"):
            text = item['text']
            tokens = tokenizer.encode(text, add_special_tokens=True)
            
            # Split into chunks with overlap
            stride = max_length // 2
            for i in range(0, max(1, len(tokens) - max_length), stride):
                chunk = tokens[i:i + max_length + 1]
                if len(chunk) >= max_length // 2:
                    self.examples.append(chunk)
        
        logger.info(f"Created {len(self.examples):,} training examples")
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        tokens = self.examples[idx]
        
        # Pad if needed
        if len(tokens) < self.max_length + 1:
            tokens = tokens + [self.tokenizer.pad_token_id or 0] * (self.max_length + 1 - len(tokens))
        else:
            tokens = tokens[:self.max_length + 1]
        
        tokens = torch.tensor(tokens, dtype=torch.long)
        return tokens[:-1], tokens[1:]


def get_lr(step: int, warmup_steps: int, max_steps: int, max_lr: float, min_lr: float = None) -> float:
    """Cosine learning rate schedule with warmup."""
    if min_lr is None:
        min_lr = max_lr * 0.1
    
    if step < warmup_steps:
        return max_lr * step / warmup_steps
    
    if step >= max_steps:
        return min_lr
    
    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (max_lr - min_lr)


def train(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Training on: {device}")
    
    if torch.cuda.is_available():
        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load tokenizer
    logger.info("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    vocab_size = len(tokenizer)
    logger.info(f"Vocab size: {vocab_size}")
    
    # Load TinyStories dataset
    logger.info("Loading TinyStories dataset...")
    dataset = load_dataset("roneneldan/TinyStories", split="train")
    
    # Use subset for faster training
    if args.max_samples:
        dataset = dataset.select(range(min(args.max_samples, len(dataset))))
    
    logger.info(f"Dataset size: {len(dataset):,} stories")
    
    # Split
    split = dataset.train_test_split(test_size=0.02, seed=42)
    train_data = split['train']
    val_data = split['test']
    
    # Create datasets
    train_dataset = TinyStoriesDataset(train_data, tokenizer, args.max_seq_len)
    val_dataset = TinyStoriesDataset(val_data, tokenizer, args.max_seq_len)
    
    logger.info(f"Train examples: {len(train_dataset):,}")
    logger.info(f"Val examples: {len(val_dataset):,}")
    
    # DataLoaders
    train_loader = DataLoader(
        train_dataset, batch_size=args.batch_size, shuffle=True,
        num_workers=4, pin_memory=True, drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2
    )
    
    # Create model
    logger.info("Creating model...")
    config = OptimizedPulseConfig(
        vocab_size=vocab_size,
        hidden_size=args.hidden_size,
        num_layers=args.num_layers,
        num_heads=args.num_heads,
        num_states=args.num_states,
        state_dim=args.hidden_size,
        intermediate_size=args.hidden_size * 4,
        max_position_embeddings=args.max_seq_len + 64,
        dropout=0.1,
    )
    
    model = OptimizedPulseForCausalLM(config).to(device)
    params = sum(p.numel() for p in model.parameters())
    logger.info(f"Parameters: {params:,} ({params * 4 / 1024**2:.1f} MB)")
    
    # Save config
    with open(output_dir / "config.json", "w") as f:
        json.dump(config.to_dict(), f, indent=2)
    
    # Optimizer with weight decay
    decay_params = []
    no_decay_params = []
    for name, param in model.named_parameters():
        if 'bias' in name or 'norm' in name or 'embedding' in name:
            no_decay_params.append(param)
        else:
            decay_params.append(param)
    
    optimizer = torch.optim.AdamW([
        {'params': decay_params, 'weight_decay': 0.1},
        {'params': no_decay_params, 'weight_decay': 0.0},
    ], lr=args.learning_rate, betas=(0.9, 0.95), eps=1e-8)
    
    scaler = GradScaler('cuda')
    
    # Training
    logger.info("=" * 70)
    logger.info("Starting training...")
    logger.info(f"  Batch size: {args.batch_size}")
    logger.info(f"  Gradient accumulation: {args.gradient_accumulation}")
    logger.info(f"  Effective batch: {args.batch_size * args.gradient_accumulation}")
    logger.info(f"  Max steps: {args.max_steps}")
    logger.info(f"  Learning rate: {args.learning_rate}")
    logger.info("=" * 70)
    
    model.train()
    best_val_loss = float('inf')
    step = 0
    accum_loss = 0
    data_iter = iter(train_loader)
    
    start_time = time.time()
    tokens_processed = 0
    
    pbar = tqdm(total=args.max_steps, desc="Training")
    
    optimizer.zero_grad()
    
    while step < args.max_steps:
        # Get batch
        try:
            x, y = next(data_iter)
        except StopIteration:
            data_iter = iter(train_loader)
            x, y = next(data_iter)
        
        x, y = x.to(device), y.to(device)
        tokens_processed += x.numel()
        
        # Update LR
        lr = get_lr(step, args.warmup_steps, args.max_steps, args.learning_rate)
        for pg in optimizer.param_groups:
            pg['lr'] = lr
        
        # Forward with mixed precision
        with autocast('cuda'):
            outputs = model(x, labels=y)
            loss = outputs['loss'] / args.gradient_accumulation
        
        scaler.scale(loss).backward()
        accum_loss += outputs['loss'].item()
        
        # Gradient accumulation step
        if (step + 1) % args.gradient_accumulation == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
        
        step += 1
        
        # Update progress
        if step % 10 == 0:
            avg_loss = accum_loss / 10
            accum_loss = 0
            elapsed = time.time() - start_time
            tok_per_sec = tokens_processed / elapsed
            
            pbar.set_postfix({
                'loss': f'{avg_loss:.3f}',
                'ppl': f'{math.exp(min(avg_loss, 10)):.1f}',
                'lr': f'{lr:.1e}',
                'tok/s': f'{tok_per_sec:.0f}'
            })
        pbar.update(1)
        
        # Evaluate
        if step % args.eval_interval == 0:
            model.eval()
            val_losses = []
            
            with torch.no_grad():
                for vx, vy in val_loader:
                    vx, vy = vx.to(device), vy.to(device)
                    with autocast('cuda'):
                        vout = model(vx, labels=vy)
                    val_losses.append(vout['loss'].item())
                    if len(val_losses) >= 100:
                        break
            
            val_loss = sum(val_losses) / len(val_losses)
            val_ppl = math.exp(min(val_loss, 10))
            
            logger.info(f"\nStep {step}: val_loss={val_loss:.4f}, val_ppl={val_ppl:.2f}")
            
            # Generate sample
            sample = generate_sample(model, tokenizer, device, "Once upon a time")
            logger.info(f"Sample: {sample}")
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'step': step,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'config': config.to_dict(),
                    'val_loss': val_loss,
                    'tokenizer_name': 'gpt2',
                }, output_dir / "best_model.pt")
                logger.info(f"Saved best model! (val_loss={val_loss:.4f})")
            
            # Also save checkpoint
            torch.save({
                'step': step,
                'model_state_dict': model.state_dict(),
                'config': config.to_dict(),
                'val_loss': val_loss,
            }, output_dir / f"checkpoint_{step}.pt")
            
            model.train()
    
    pbar.close()
    
    elapsed = time.time() - start_time
    logger.info("=" * 70)
    logger.info(f"Training complete!")
    logger.info(f"  Time: {elapsed/3600:.2f} hours")
    logger.info(f"  Best val loss: {best_val_loss:.4f}")
    logger.info(f"  Best val PPL: {math.exp(best_val_loss):.2f}")
    logger.info(f"  Tokens processed: {tokens_processed:,}")
    logger.info(f"  Throughput: {tokens_processed/elapsed:.0f} tok/s")
    logger.info("=" * 70)
    
    # Final generation samples
    logger.info("\nFinal generation samples:")
    model.eval()
    prompts = [
        "Once upon a time",
        "The little girl",
        "Tom was a boy who",
        "In the forest",
    ]
    for prompt in prompts:
        sample = generate_sample(model, tokenizer, device, prompt, max_tokens=100)
        logger.info(f"\n'{prompt}':\n{sample}")


@torch.no_grad()
def generate_sample(model, tokenizer, device, prompt, max_tokens=80, temperature=0.8):
    """Generate text sample."""
    model.eval()
    
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    
    for _ in range(max_tokens):
        if input_ids.shape[1] >= 250:
            break
        
        with autocast('cuda'):
            outputs = model(input_ids)
        
        logits = outputs['logits'][:, -1, :] / temperature
        
        # Top-k + top-p sampling
        top_k = 50
        top_p = 0.9
        
        # Top-k
        topk_vals, topk_idx = torch.topk(logits, top_k)
        logits[logits < topk_vals[:, -1:]] = float('-inf')
        
        # Top-p
        sorted_logits, sorted_idx = torch.sort(logits, descending=True)
        cumsum = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        remove_mask = cumsum > top_p
        remove_mask[:, 1:] = remove_mask[:, :-1].clone()
        remove_mask[:, 0] = False
        sorted_logits[remove_mask] = float('-inf')
        logits = sorted_logits.scatter(1, sorted_idx, sorted_logits)
        
        probs = F.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        
        input_ids = torch.cat([input_ids, next_token], dim=1)
        
        # Stop at EOS
        if next_token.item() == tokenizer.eos_token_id:
            break
    
    return tokenizer.decode(input_ids[0], skip_special_tokens=True)


def main():
    parser = argparse.ArgumentParser(description="Train PULSE on TinyStories")
    
    # Model
    parser.add_argument("--hidden-size", type=int, default=512)
    parser.add_argument("--num-layers", type=int, default=8)
    parser.add_argument("--num-heads", type=int, default=8)
    parser.add_argument("--num-states", type=int, default=32)
    parser.add_argument("--max-seq-len", type=int, default=256)
    
    # Training
    parser.add_argument("--batch-size", type=int, default=32)
    parser.add_argument("--gradient-accumulation", type=int, default=2)
    parser.add_argument("--learning-rate", type=float, default=5e-4)
    parser.add_argument("--max-steps", type=int, default=20000)
    parser.add_argument("--warmup-steps", type=int, default=1000)
    parser.add_argument("--eval-interval", type=int, default=1000)
    
    # Data
    parser.add_argument("--max-samples", type=int, default=100000,
                        help="Max stories to use (None for all)")
    
    # Output
    parser.add_argument("--output-dir", type=str, default="./output/pulse_tinystories")
    
    args = parser.parse_args()
    train(args)


if __name__ == "__main__":
    main()
