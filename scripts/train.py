#!/usr/bin/env python3
"""
Train PULSE on TinyStories dataset from Hugging Face.

TinyStories contains ~2.1M synthetic short stories generated by GPT-3.5/4.
This is ideal for training small language models that can generate coherent text.
"""

import argparse
import gc
import json
import logging
import math
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Union

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.amp import GradScaler, autocast
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

from datasets import load_dataset
from transformers import AutoTokenizer

from pulse import PulseConfig, PulseForCausalLM
from pulse.utils.config import load_config, FullConfig, TrainingConfig, DataConfig, OutputConfig

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)


class TinyStoriesDataset(Dataset):
    """Dataset wrapper for TinyStories."""
    
    def __init__(self, data, tokenizer, max_length: int = 256):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.examples = []
        
        logger.info("Tokenizing dataset...")
        for item in tqdm(data, desc="Processing"):
            text = item['text']
            tokens = tokenizer.encode(text, add_special_tokens=True)
            
            # Split into chunks with overlap
            stride = max_length // 2
            for i in range(0, max(1, len(tokens) - max_length), stride):
                chunk = tokens[i:i + max_length + 1]
                if len(chunk) >= max_length // 2:
                    self.examples.append(chunk)
        
        logger.info(f"Created {len(self.examples):,} training examples")
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        tokens = self.examples[idx]
        
        # Pad if needed
        if len(tokens) < self.max_length + 1:
            tokens = tokens + [self.tokenizer.pad_token_id or 0] * (self.max_length + 1 - len(tokens))
        else:
            tokens = tokens[:self.max_length + 1]
        
        tokens = torch.tensor(tokens, dtype=torch.long)
        return tokens[:-1], tokens[1:]


def get_lr(step: int, warmup_steps: int, max_steps: int, max_lr: float, min_lr: float = None) -> float:
    """Cosine learning rate schedule with warmup."""
    if min_lr is None:
        min_lr = max_lr * 0.1
    
    if step < warmup_steps:
        return max_lr * step / warmup_steps
    
    if step >= max_steps:
        return min_lr
    
    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (max_lr - min_lr)


def train(args, config: Optional[FullConfig] = None):
    """Main training function."""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Training on: {device}")
    
    if torch.cuda.is_available():
        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # Use config if provided, otherwise use args
    if config is not None:
        model_config = config.model
        train_cfg = config.training
        data_cfg = config.data
        output_cfg = config.output
        seed = config.seed
    else:
        # Fallback to args (backward compatibility)
        model_config = None
        train_cfg = None
        data_cfg = None
        output_cfg = None
        seed = 42
    
    # Set seed
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    # Get values from config or args
    output_dir = Path(output_cfg.output_dir if output_cfg else args.output_dir)
    batch_size = train_cfg.batch_size if train_cfg else args.batch_size
    gradient_accumulation = train_cfg.gradient_accumulation_steps if train_cfg else args.gradient_accumulation
    learning_rate = train_cfg.learning_rate if train_cfg else args.learning_rate
    max_steps = train_cfg.max_steps if train_cfg and train_cfg.max_steps else args.max_steps
    warmup_steps = train_cfg.warmup_steps if train_cfg else args.warmup_steps
    eval_steps = train_cfg.eval_steps if train_cfg else args.eval_interval
    max_seq_len = data_cfg.max_seq_length if data_cfg else args.max_seq_len
    max_samples = data_cfg.max_samples if data_cfg else args.max_samples
    
    # Mixed precision settings
    use_fp16 = train_cfg.fp16 if train_cfg else False
    use_bf16 = train_cfg.bf16 if train_cfg else False
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load tokenizer
    logger.info("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    vocab_size = len(tokenizer)
    logger.info(f"Vocab size: {vocab_size}")
    
    # Load TinyStories dataset
    logger.info("Loading TinyStories dataset...")
    dataset = load_dataset("roneneldan/TinyStories", split="train")
    
    # Use subset for faster training
    if max_samples:
        dataset = dataset.select(range(min(max_samples, len(dataset))))
    
    logger.info(f"Dataset size: {len(dataset):,} stories")
    
    # Split
    split = dataset.train_test_split(test_size=0.02, seed=seed)
    train_data = split['train']
    val_data = split['test']
    
    # Create datasets
    train_dataset = TinyStoriesDataset(train_data, tokenizer, max_seq_len)
    val_dataset = TinyStoriesDataset(val_data, tokenizer, max_seq_len)
    
    logger.info(f"Train examples: {len(train_dataset):,}")
    logger.info(f"Val examples: {len(val_dataset):,}")
    
    # DataLoaders
    num_workers = data_cfg.dataloader_num_workers if data_cfg else 4
    pin_memory = data_cfg.dataloader_pin_memory if data_cfg else True
    
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True,
        num_workers=num_workers, pin_memory=pin_memory, drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False, num_workers=2
    )
    
    # Create model
    logger.info("Creating model...")
    if model_config is not None:
        # Override vocab_size from tokenizer
        model_config.vocab_size = vocab_size
        pulse_config = model_config
    else:
        # Minimal, current PULSE configuration.
        # Recurrent state is enabled by default; external memory is left off so that
        # ablation runs can cleanly measure its impact.
        pulse_config = PulseConfig(
            vocab_size=vocab_size,
            hidden_size=args.hidden_size,
            num_layers=args.num_layers,
            num_heads=args.num_heads,
            max_seq_len=args.max_seq_len,
            use_recurrent_state=True,
            use_memory=False,
            dropout=0.1,
        )
    
    model = PulseForCausalLM(pulse_config).to(device)
    params = sum(p.numel() for p in model.parameters())
    logger.info(f"Parameters: {params:,} ({params * 4 / 1024**2:.1f} MB)")
    
    # Save config
    with open(output_dir / "config.json", "w") as f:
        json.dump(pulse_config.to_dict(), f, indent=2)
    
    # Optimizer with weight decay
    weight_decay = train_cfg.weight_decay if train_cfg else 0.1
    decay_params = []
    no_decay_params = []
    for name, param in model.named_parameters():
        if 'bias' in name or 'norm' in name or 'embedding' in name:
            no_decay_params.append(param)
        else:
            decay_params.append(param)
    
    optimizer = torch.optim.AdamW([
        {'params': decay_params, 'weight_decay': weight_decay},
        {'params': no_decay_params, 'weight_decay': 0.0},
    ], lr=learning_rate, betas=(0.9, 0.95), eps=1e-8)
    
    # Mixed precision scaler
    if use_bf16:
        scaler = None  # BF16 doesn't need scaler
        autocast_dtype = torch.bfloat16
    elif use_fp16:
        scaler = GradScaler('cuda')
        autocast_dtype = torch.float16
    else:
        scaler = GradScaler('cuda')
        autocast_dtype = torch.float16  # Default to FP16
    
    # Resume from checkpoint if specified
    start_step = 0
    best_val_loss = float('inf')
    
    if args.resume:
        resume_path = Path(args.resume)
        if resume_path.exists():
            logger.info(f"Resuming from checkpoint: {resume_path}")
            checkpoint = torch.load(resume_path, map_location=device)
            
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            if scaler is not None and checkpoint.get('scaler_state_dict'):
                scaler.load_state_dict(checkpoint['scaler_state_dict'])
            
            start_step = checkpoint.get('step', 0)
            best_val_loss = checkpoint.get('val_loss', float('inf'))
            
            logger.info(f"  Resumed from step {start_step}")
            logger.info(f"  Best val loss so far: {best_val_loss:.4f}")
        else:
            logger.warning(f"Checkpoint not found: {resume_path}, starting from scratch")
    
    # Training
    logger.info("=" * 70)
    logger.info("Starting training...")
    logger.info(f"  Batch size: {batch_size}")
    logger.info(f"  Gradient accumulation: {gradient_accumulation}")
    logger.info(f"  Effective batch: {batch_size * gradient_accumulation}")
    logger.info(f"  Max steps: {max_steps}")
    logger.info(f"  Learning rate: {learning_rate}")
    logger.info(f"  Mixed precision: {'BF16' if use_bf16 else 'FP16' if use_fp16 else 'FP16 (default)'}")
    logger.info("=" * 70)
    
    model.train()
    # Use values from checkpoint if resuming
    step = start_step
    accum_loss = 0
    data_iter = iter(train_loader)
    
    start_time = time.time()
    tokens_processed = 0
    
    pbar = tqdm(total=max_steps, initial=start_step, desc="Training")
    
    optimizer.zero_grad()
    
    while step < max_steps:
        # Get batch
        try:
            x, y = next(data_iter)
        except StopIteration:
            data_iter = iter(train_loader)
            x, y = next(data_iter)
        
        x, y = x.to(device), y.to(device)
        tokens_processed += x.numel()
        
        # Update LR
        lr = get_lr(step, warmup_steps, max_steps, learning_rate)
        for pg in optimizer.param_groups:
            pg['lr'] = lr
        
        # Forward with mixed precision
        with autocast('cuda', dtype=autocast_dtype):
            outputs = model(x, labels=y)
            loss = outputs['loss'] / gradient_accumulation
        
        if scaler is not None:
            scaler.scale(loss).backward()
        else:
            loss.backward()
        accum_loss += outputs['loss'].item()
        
        # Gradient accumulation step
        max_grad_norm = train_cfg.max_grad_norm if train_cfg else 1.0
        if (step + 1) % gradient_accumulation == 0:
            if scaler is not None:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                scaler.step(optimizer)
                scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                optimizer.step()
            optimizer.zero_grad()
        
        step += 1
        
        # Update progress
        if step % 10 == 0:
            avg_loss = accum_loss / 10
            accum_loss = 0
            elapsed = time.time() - start_time
            tok_per_sec = tokens_processed / elapsed
            
            pbar.set_postfix({
                'loss': f'{avg_loss:.3f}',
                'ppl': f'{math.exp(min(avg_loss, 10)):.1f}',
                'lr': f'{lr:.1e}',
                'tok/s': f'{tok_per_sec:.0f}'
            })
        pbar.update(1)
        
        # Evaluate
        if step % eval_steps == 0:
            model.eval()
            val_losses = []
            
            with torch.no_grad():
                for vx, vy in val_loader:
                    vx, vy = vx.to(device), vy.to(device)
                    with autocast('cuda'):
                        vout = model(vx, labels=vy)
                    val_losses.append(vout['loss'].item())
                    if len(val_losses) >= 100:
                        break
            
            val_loss = sum(val_losses) / len(val_losses)
            val_ppl = math.exp(min(val_loss, 10))
            
            logger.info(f"\nStep {step}: val_loss={val_loss:.4f}, val_ppl={val_ppl:.2f}")
            
            # Generate sample
            sample = generate_sample(model, tokenizer, device, "Once upon a time")
            logger.info(f"Sample: {sample}")
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'step': step,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scaler_state_dict': scaler.state_dict() if scaler else None,
                    'config': pulse_config.to_dict(),
                    'val_loss': val_loss,
                    'tokenizer_name': 'gpt2',
                }, output_dir / "best_model.pt")
                logger.info(f"Saved best model! (val_loss={val_loss:.4f})")
            
            # Also save checkpoint
            torch.save({
                'step': step,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scaler_state_dict': scaler.state_dict() if scaler else None,
                'config': pulse_config.to_dict(),
                'val_loss': val_loss,
            }, output_dir / f"checkpoint_{step}.pt")
            
            model.train()
    
    pbar.close()
    
    elapsed = time.time() - start_time
    logger.info("=" * 70)
    logger.info(f"Training complete!")
    logger.info(f"  Time: {elapsed/3600:.2f} hours")
    logger.info(f"  Best val loss: {best_val_loss:.4f}")
    logger.info(f"  Best val PPL: {math.exp(best_val_loss):.2f}")
    logger.info(f"  Tokens processed: {tokens_processed:,}")
    logger.info(f"  Throughput: {tokens_processed/elapsed:.0f} tok/s")
    logger.info("=" * 70)
    
    # Final generation samples
    logger.info("\nFinal generation samples:")
    model.eval()
    prompts = [
        "Once upon a time",
        "The little girl",
        "Tom was a boy who",
        "In the forest",
    ]
    for prompt in prompts:
        sample = generate_sample(model, tokenizer, device, prompt, max_tokens=100)
        logger.info(f"\n'{prompt}':\n{sample}")


@torch.no_grad()
def generate_sample(model, tokenizer, device, prompt, max_tokens=80, temperature=0.8):
    """Generate text sample."""
    model.eval()
    
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    
    for _ in range(max_tokens):
        if input_ids.shape[1] >= 250:
            break
        
        with autocast('cuda'):
            outputs = model(input_ids)
        
        logits = outputs['logits'][:, -1, :] / temperature
        
        # Top-k + top-p sampling
        top_k = 50
        top_p = 0.9
        
        # Top-k
        topk_vals, topk_idx = torch.topk(logits, top_k)
        logits[logits < topk_vals[:, -1:]] = float('-inf')
        
        # Top-p
        sorted_logits, sorted_idx = torch.sort(logits, descending=True)
        cumsum = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        remove_mask = cumsum > top_p
        remove_mask[:, 1:] = remove_mask[:, :-1].clone()
        remove_mask[:, 0] = False
        sorted_logits[remove_mask] = float('-inf')
        logits = sorted_logits.scatter(1, sorted_idx, sorted_logits)
        
        probs = F.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        
        input_ids = torch.cat([input_ids, next_token], dim=1)
        
        # Stop at EOS
        if next_token.item() == tokenizer.eos_token_id:
            break
    
    return tokenizer.decode(input_ids[0], skip_special_tokens=True)


def main():
    parser = argparse.ArgumentParser(description="Train PULSE on TinyStories")
    
    # Config file (preferred)
    parser.add_argument("--config", type=str, default=None,
                        help="Path to YAML config file (e.g., configs/pulse_small.yaml)")
    
    # Model (used if no config file)
    parser.add_argument("--hidden-size", type=int, default=512)
    parser.add_argument("--num-layers", type=int, default=8)
    parser.add_argument("--num-heads", type=int, default=8)
    parser.add_argument("--max-seq-len", type=int, default=256)
    
    # Training (used if no config file)
    parser.add_argument("--batch-size", type=int, default=32)
    parser.add_argument("--gradient-accumulation", type=int, default=2)
    parser.add_argument("--learning-rate", type=float, default=5e-4)
    parser.add_argument("--max-steps", type=int, default=20000)
    parser.add_argument("--warmup-steps", type=int, default=1000)
    parser.add_argument("--eval-interval", type=int, default=1000)
    
    # Data (used if no config file)
    parser.add_argument("--max-samples", type=int, default=100000,
                        help="Max stories to use (None for all)")
    
    # Output (used if no config file)
    parser.add_argument("--output-dir", type=str, default="./output/pulse_tinystories")
    
    # Resume training
    parser.add_argument("--resume", type=str, default=None,
                        help="Path to checkpoint to resume from")
    
    args = parser.parse_args()
    
    # Load config if provided
    config = None
    if args.config:
        logger.info(f"Loading config from: {args.config}")
        config = load_config(args.config)
    
    train(args, config=config)


if __name__ == "__main__":
    main()
